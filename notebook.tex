
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{AI-26}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{House Prices: Advanced Regression
Techniques}\label{house-prices-advanced-regression-techniques}

    \textbf{Name}: AI 0026 \textbf{Submission Date}: 13.8.2018

    \section{Introduction}\label{introduction}

This assignment is based on the prediction of Sale price of house with
the given features

    \section{Step by step procedure}\label{step-by-step-procedure}

Importing Required Libraries

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k}{import} \PY{n}{norm}
        \PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k}{import} \PY{n}{stats}
\end{Verbatim}


    \section{Exploratory Data Analysis}\label{exploratory-data-analysis}

This file provide a basic exploration of ames house price dataset

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./input/train.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:}    Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \textbackslash{}
        0   1          60       RL         65.0     8450   Pave   NaN      Reg   
        1   2          20       RL         80.0     9600   Pave   NaN      Reg   
        2   3          60       RL         68.0    11250   Pave   NaN      IR1   
        3   4          70       RL         60.0     9550   Pave   NaN      IR1   
        4   5          60       RL         84.0    14260   Pave   NaN      IR1   
        
          LandContour Utilities    {\ldots}     PoolArea PoolQC Fence MiscFeature MiscVal  \textbackslash{}
        0         Lvl    AllPub    {\ldots}            0    NaN   NaN         NaN       0   
        1         Lvl    AllPub    {\ldots}            0    NaN   NaN         NaN       0   
        2         Lvl    AllPub    {\ldots}            0    NaN   NaN         NaN       0   
        3         Lvl    AllPub    {\ldots}            0    NaN   NaN         NaN       0   
        4         Lvl    AllPub    {\ldots}            0    NaN   NaN         NaN       0   
        
          MoSold YrSold  SaleType  SaleCondition  SalePrice  
        0      2   2008        WD         Normal     208500  
        1      5   2007        WD         Normal     181500  
        2      9   2008        WD         Normal     223500  
        3      2   2006        WD        Abnorml     140000  
        4     12   2008        WD         Normal     250000  
        
        [5 rows x 81 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:}                 Id   MSSubClass  LotFrontage        LotArea  OverallQual  \textbackslash{}
        count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   
        mean    730.500000    56.897260    70.049958   10516.828082     6.099315   
        std     421.610009    42.300571    24.284752    9981.264932     1.382997   
        min       1.000000    20.000000    21.000000    1300.000000     1.000000   
        25\%     365.750000    20.000000    59.000000    7553.500000     5.000000   
        50\%     730.500000    50.000000    69.000000    9478.500000     6.000000   
        75\%    1095.250000    70.000000    80.000000   11601.500000     7.000000   
        max    1460.000000   190.000000   313.000000  215245.000000    10.000000   
        
               OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  \textbackslash{}
        count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000   
        mean      5.575342  1971.267808   1984.865753   103.685262   443.639726   
        std       1.112799    30.202904     20.645407   181.066207   456.098091   
        min       1.000000  1872.000000   1950.000000     0.000000     0.000000   
        25\%       5.000000  1954.000000   1967.000000     0.000000     0.000000   
        50\%       5.000000  1973.000000   1994.000000     0.000000   383.500000   
        75\%       6.000000  2000.000000   2004.000000   166.000000   712.250000   
        max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000   
        
                   {\ldots}         WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \textbackslash{}
        count      {\ldots}        1460.000000  1460.000000    1460.000000  1460.000000   
        mean       {\ldots}          94.244521    46.660274      21.954110     3.409589   
        std        {\ldots}         125.338794    66.256028      61.119149    29.317331   
        min        {\ldots}           0.000000     0.000000       0.000000     0.000000   
        25\%        {\ldots}           0.000000     0.000000       0.000000     0.000000   
        50\%        {\ldots}           0.000000    25.000000       0.000000     0.000000   
        75\%        {\ldots}         168.000000    68.000000       0.000000     0.000000   
        max        {\ldots}         857.000000   547.000000     552.000000   508.000000   
        
               ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \textbackslash{}
        count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000   
        mean     15.060959     2.758904     43.489041     6.321918  2007.815753   
        std      55.757415    40.177307    496.123024     2.703626     1.328095   
        min       0.000000     0.000000      0.000000     1.000000  2006.000000   
        25\%       0.000000     0.000000      0.000000     5.000000  2007.000000   
        50\%       0.000000     0.000000      0.000000     6.000000  2008.000000   
        75\%       0.000000     0.000000      0.000000     8.000000  2009.000000   
        max     480.000000   738.000000  15500.000000    12.000000  2010.000000   
        
                   SalePrice  
        count    1460.000000  
        mean   180921.195890  
        std     79442.502883  
        min     34900.000000  
        25\%    129975.000000  
        50\%    163000.000000  
        75\%    214000.000000  
        max    755000.000000  
        
        [8 rows x 38 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} Set up the matplotlib figure}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}f, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{121}\PY{p}{)}
        \PY{n}{sns}\PY{o}{.}\PY{n}{distplot}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{kde}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sale price}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{800000}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{180}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{122}\PY{p}{)}
        \PY{n}{sns}\PY{o}{.}\PY{n}{distplot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{kde}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Log (sale price)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{14}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{180}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}axes\textbackslash{}\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.
  warnings.warn("The 'normed' kwarg is deprecated, and has been "
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}axes\textbackslash{}\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.
  warnings.warn("The 'normed' kwarg is deprecated, and has been "

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} [10, 14, 0, 180]
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_9_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{corr} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{select\PYZus{}dtypes}\PY{p}{(}\PY{n}{include} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float64}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{int64}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}fig = plt.figure()}
        \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{font\PYZus{}scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  
        \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{corr}\PY{p}{,} \PY{n}{vmax}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{square}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x1f97d988da0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{corr\PYZus{}list} \PY{o}{=} \PY{n}{corr}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
        \PY{n}{corr\PYZus{}list}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:} OverallQual      0.790982
        GrLivArea        0.708624
        GarageCars       0.640409
        GarageArea       0.623431
        TotalBsmtSF      0.613581
        1stFlrSF         0.605852
        FullBath         0.560664
        TotRmsAbvGrd     0.533723
        YearBuilt        0.522897
        YearRemodAdd     0.507101
        GarageYrBlt      0.486362
        MasVnrArea       0.477493
        Fireplaces       0.466929
        BsmtFinSF1       0.386420
        LotFrontage      0.351799
        WoodDeckSF       0.324413
        2ndFlrSF         0.319334
        OpenPorchSF      0.315856
        HalfBath         0.284108
        LotArea          0.263843
        BsmtFullBath     0.227122
        BsmtUnfSF        0.214479
        BedroomAbvGr     0.168213
        ScreenPorch      0.111447
        PoolArea         0.092404
        MoSold           0.046432
        3SsnPorch        0.044584
        BsmtFinSF2      -0.011378
        BsmtHalfBath    -0.016844
        MiscVal         -0.021190
        LowQualFinSF    -0.025606
        YrSold          -0.028923
        OverallCond     -0.077856
        MSSubClass      -0.084284
        EnclosedPorch   -0.128578
        KitchenAbvGr    -0.135907
        Name: SalePrice, dtype: float64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{:}
            \PY{n}{ii} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{23}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{n}{ii}\PY{p}{)}
            \PY{n}{feature} \PY{o}{=} \PY{n}{corr\PYZus{}list}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{facecolors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{none}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{edgecolors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{s} \PY{o}{=} \PY{l+m+mi}{75}\PY{p}{)}
            \PY{n}{sns}\PY{o}{.}\PY{n}{regplot}\PY{p}{(}\PY{n}{x} \PY{o}{=} \PY{n}{feature}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data} \PY{o}{=} \PY{n}{df}\PY{p}{,}\PY{n}{scatter}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{ax}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)} 
            \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{800000}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
         \PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Neighborhood}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{n}{data} \PY{o}{=} \PY{n}{df}\PY{p}{)}
         \PY{n}{xt} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Feature Selection}\label{feature-selection}

Importing Required Libraries

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{import} \PY{n+nn}{datetime}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cross\PYZus{}validation} \PY{k}{import} \PY{n}{KFold}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cross\PYZus{}validation} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         \PY{k+kn}{import} \PY{n+nn}{time}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing}
         \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k}{import} \PY{n}{skew}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}cross\_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model\_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./input/train.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} read train data}
         \PY{n}{test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./input/test.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} read test data}
         
         \PY{n}{tables} \PY{o}{=} \PY{p}{[}\PY{n}{train}\PY{p}{,}\PY{n}{test}\PY{p}{]}
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Delete features with high number of missing values...}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{total\PYZus{}missing} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
         \PY{n}{to\PYZus{}delete} \PY{o}{=} \PY{n}{total\PYZus{}missing}\PY{p}{[}\PY{n}{total\PYZus{}missing}\PY{o}{\PYZgt{}}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{/}\PY{l+m+mf}{3.}\PY{p}{)}\PY{p}{]}
         \PY{k}{for} \PY{n}{table} \PY{o+ow}{in} \PY{n}{tables}\PY{p}{:}
             \PY{n}{table}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{to\PYZus{}delete}\PY{o}{.}\PY{n}{index}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{numerical\PYZus{}features} \PY{o}{=} \PY{n}{test}\PY{o}{.}\PY{n}{select\PYZus{}dtypes}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{float}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{int}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bool}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
         \PY{n}{categorical\PYZus{}features} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{select\PYZus{}dtypes}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{object}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Delete features with high number of missing values{\ldots}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{to\PYZus{}delete}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:} Alley          1369
         FireplaceQu     690
         PoolQC         1453
         Fence          1179
         MiscFeature    1406
         dtype: int64
\end{Verbatim}
            
    \section{Outlier Detection}\label{outlier-detection}

Importing Required Libraries

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{svm}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{covariance} \PY{k}{import} \PY{n}{EllipticEnvelope}
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./input/train.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{n}{test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./input/test.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
         \PY{n}{origin} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{dif} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{test}\PY{o}{\PYZhy{}}\PY{n}{origin}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{12000}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{idx} \PY{o}{=} \PY{n}{dif}\PY{p}{[}\PY{n}{dif}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{,}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{train}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}20}]:} (1460, 81)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{idx}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:} []
\end{Verbatim}
            
    \section{DESIGNING KERAS MODEL}\label{designing-keras-model}

LABELENCODING TO CONVERT CATEGORICAL DATA TO NUMERICAL DATA

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{data1}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./input/train.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{test}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./input/test.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{test1}\PY{o}{=}\PY{n}{test}
         \PY{n}{data}\PY{o}{=}\PY{n}{data1}
         \PY{n}{data}\PY{o}{=}\PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Id}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SalePrice}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{test1}\PY{o}{=}\PY{n}{test1}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Id}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{data}\PY{p}{,}\PY{n}{test1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Utilities}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PoolQC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MiscFeature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alley}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fence}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FireplaceQu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MasVnrType}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MasVnrArea}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
             \PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{None}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GarageType}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GarageFinish}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GarageQual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GarageCond}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{None}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtQual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtCond}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtExposure}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtFinType1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtFinType2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{None}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GarageYrBlt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GarageArea}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GarageCars}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]} \PY{o}{=}\PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtFinSF1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtFinSF2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtUnfSF}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TotalBsmtSF}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtFullBath}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BsmtHalfBath}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSZoning}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSZoning}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSZoning}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Functional}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=}\PY{n}{data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Functional}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Typ}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Electrical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Electrical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Electrical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{KitchenQual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{KitchenQual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{KitchenQual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Exterior1st}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Exterior1st}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Exterior1st}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Exterior2nd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Exterior2nd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Exterior2nd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SaleType}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SaleType}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SaleType}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSSubClass}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSSubClass}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{None}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LotFrontage}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Neighborhood}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LotFrontage}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n}{data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{GrLivArea}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{GrLivArea}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{1stFlrSF}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{1stFlrSF}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{LabelEncoder}
         \PY{n}{cols}\PY{o}{=}\PY{n}{data}\PY{o}{.}\PY{n}{columns}
         \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{cols}\PY{p}{:}
             \PY{n}{lbl} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)} 
             \PY{n}{lbl}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{c}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{)} 
             \PY{n}{data}\PY{p}{[}\PY{n}{c}\PY{p}{]} \PY{o}{=} \PY{n}{lbl}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{c}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{trainSet}\PY{o}{=}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data1}\PY{p}{)}\PY{p}{]}
         \PY{n}{target}\PY{o}{=}\PY{n}{data1}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SalePrice}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{testSet}\PY{o}{=}\PY{n}{data}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data1}\PY{p}{)}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{]}
         \PY{n}{x}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{trainSet}\PY{p}{)}
         \PY{n}{y}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{target}\PY{p}{)}
         \PY{n}{testSet1}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{testSet}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Activation}\PY{p}{,} \PY{n}{Dense}\PY{p}{,} \PY{n}{Flatten}\PY{p}{,} \PY{n}{Dropout}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{optimizers} \PY{k}{import} \PY{n}{SGD}\PY{p}{,}\PY{n}{Adagrad}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{categorical\PYZus{}crossentropy}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Using TensorFlow backend.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{backend} \PY{k}{as} \PY{n}{K}
         
         \PY{k}{def} \PY{n+nf}{coeff\PYZus{}determination}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{:}
             \PY{n}{SS\PYZus{}res} \PY{o}{=}  \PY{n}{K}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{K}\PY{o}{.}\PY{n}{square}\PY{p}{(} \PY{n}{y\PYZus{}true}\PY{o}{\PYZhy{}}\PY{n}{y\PYZus{}pred} \PY{p}{)}\PY{p}{)} 
             \PY{n}{SS\PYZus{}tot} \PY{o}{=} \PY{n}{K}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{K}\PY{o}{.}\PY{n}{square}\PY{p}{(} \PY{n}{y\PYZus{}true} \PY{o}{\PYZhy{}} \PY{n}{K}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{)} \PY{p}{)} \PY{p}{)} 
             \PY{k}{return} \PY{p}{(} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{SS\PYZus{}res}\PY{o}{/}\PY{p}{(}\PY{n}{SS\PYZus{}tot} \PY{o}{+} \PY{n}{K}\PY{o}{.}\PY{n}{epsilon}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{x}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}40}]:} (1460, 78)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{i}\PY{o}{=}\PY{l+m+mi}{1200}
         
         \PY{n}{j}\PY{o}{=}\PY{l+m+mf}{0.40}
         \PY{n}{model}\PY{o}{=}\PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{78}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{n}{j}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{n}{j}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{n}{j}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{n}{j}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{n}{j}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         
         
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mse}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{optimizer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{adam}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{n}{coeff\PYZus{}determination}\PY{p}{]}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{validation\PYZus{}split}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}\PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 1168 samples, validate on 292 samples
Epoch 1/500
1168/1168 [==============================] - 1s 1ms/step - loss: 26702500881.5342 - coeff\_determination: -3.9842 - val\_loss: 3611453552.2192 - val\_coeff\_determination: 0.4741
Epoch 2/500
1168/1168 [==============================] - 1s 1ms/step - loss: 3389553244.9315 - coeff\_determination: 0.3258 - val\_loss: 2736327448.5479 - val\_coeff\_determination: 0.6044
Epoch 3/500
1168/1168 [==============================] - 1s 1ms/step - loss: 3125811591.0137 - coeff\_determination: 0.4112 - val\_loss: 2505961210.7397 - val\_coeff\_determination: 0.6346
Epoch 4/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2871727275.8356 - coeff\_determination: 0.4751 - val\_loss: 2384828542.2466 - val\_coeff\_determination: 0.6544
Epoch 5/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2540551911.4521 - coeff\_determination: 0.5007 - val\_loss: 2312711573.0411 - val\_coeff\_determination: 0.6653
Epoch 6/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2426886412.2740 - coeff\_determination: 0.5559 - val\_loss: 2245836933.2603 - val\_coeff\_determination: 0.6822
Epoch 7/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2601971876.8219 - coeff\_determination: 0.5295 - val\_loss: 2234734662.1370 - val\_coeff\_determination: 0.6722
Epoch 8/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2502558933.9178 - coeff\_determination: 0.4889 - val\_loss: 2207150341.2603 - val\_coeff\_determination: 0.6812
Epoch 9/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2311659372.7123 - coeff\_determination: 0.5845 - val\_loss: 2264553109.0411 - val\_coeff\_determination: 0.6677
Epoch 10/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2455438500.8219 - coeff\_determination: 0.5649 - val\_loss: 2219731764.6027 - val\_coeff\_determination: 0.6930
Epoch 11/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2131092502.7945 - coeff\_determination: 0.6251 - val\_loss: 2181817873.5342 - val\_coeff\_determination: 0.6993
Epoch 12/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2572166044.0548 - coeff\_determination: 0.5441 - val\_loss: 2164794094.4658 - val\_coeff\_determination: 0.6920
Epoch 13/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2533506926.4658 - coeff\_determination: 0.5601 - val\_loss: 2127346097.0959 - val\_coeff\_determination: 0.7058
Epoch 14/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2260201512.3288 - coeff\_determination: 0.6031 - val\_loss: 2197419260.4932 - val\_coeff\_determination: 0.7003
Epoch 15/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2501987885.5890 - coeff\_determination: 0.5758 - val\_loss: 2086899571.7260 - val\_coeff\_determination: 0.7116
Epoch 16/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2499793415.0137 - coeff\_determination: 0.5963 - val\_loss: 2111471647.5616 - val\_coeff\_determination: 0.7132
Epoch 17/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2417232519.0137 - coeff\_determination: 0.5889 - val\_loss: 2092157443.5068 - val\_coeff\_determination: 0.7066
Epoch 18/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2267599489.7534 - coeff\_determination: 0.5719 - val\_loss: 2041505862.1370 - val\_coeff\_determination: 0.7187
Epoch 19/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2375314558.2466 - coeff\_determination: 0.5883 - val\_loss: 2062249827.9452 - val\_coeff\_determination: 0.7197
Epoch 20/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2474059964.0548 - coeff\_determination: 0.5716 - val\_loss: 2070612660.6027 - val\_coeff\_determination: 0.7175
Epoch 21/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2266541952.0000 - coeff\_determination: 0.5925 - val\_loss: 2071585209.8630 - val\_coeff\_determination: 0.7216
Epoch 22/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2598884611.5068 - coeff\_determination: 0.5268 - val\_loss: 2049775796.6027 - val\_coeff\_determination: 0.7228
Epoch 23/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2429406253.5890 - coeff\_determination: 0.5589 - val\_loss: 2019251217.5342 - val\_coeff\_determination: 0.7274
Epoch 24/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2279803202.6301 - coeff\_determination: 0.6042 - val\_loss: 2024603875.9452 - val\_coeff\_determination: 0.7292
Epoch 25/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2454604054.7945 - coeff\_determination: 0.5739 - val\_loss: 2000715002.7397 - val\_coeff\_determination: 0.7307
Epoch 26/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2359382273.7534 - coeff\_determination: 0.6076 - val\_loss: 2000685012.1644 - val\_coeff\_determination: 0.7256
Epoch 27/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2193534779.6164 - coeff\_determination: 0.6223 - val\_loss: 2041745269.4795 - val\_coeff\_determination: 0.7111
Epoch 28/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2217640763.6164 - coeff\_determination: 0.5971 - val\_loss: 1994737869.1507 - val\_coeff\_determination: 0.7351
Epoch 29/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2177818778.3014 - coeff\_determination: 0.6230 - val\_loss: 2036352496.2192 - val\_coeff\_determination: 0.7309
Epoch 30/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2463103014.5753 - coeff\_determination: 0.5945 - val\_loss: 1932468132.8219 - val\_coeff\_determination: 0.7406
Epoch 31/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2186266567.8904 - coeff\_determination: 0.6257 - val\_loss: 2028966892.7123 - val\_coeff\_determination: 0.7338
Epoch 32/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1994658453.0411 - coeff\_determination: 0.6488 - val\_loss: 1960093589.9178 - val\_coeff\_determination: 0.7393
Epoch 33/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2417415741.8082 - coeff\_determination: 0.5652 - val\_loss: 1963885410.1918 - val\_coeff\_determination: 0.7403
Epoch 34/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2384969207.2329 - coeff\_determination: 0.5839 - val\_loss: 1929823393.3151 - val\_coeff\_determination: 0.7436
Epoch 35/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2062691186.8493 - coeff\_determination: 0.6354 - val\_loss: 1946193507.0685 - val\_coeff\_determination: 0.7437
Epoch 36/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2040249673.6438 - coeff\_determination: 0.6358 - val\_loss: 1981048835.5068 - val\_coeff\_determination: 0.7404
Epoch 37/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2129672318.2466 - coeff\_determination: 0.6200 - val\_loss: 2024114374.1370 - val\_coeff\_determination: 0.7341
Epoch 38/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2037902297.4247 - coeff\_determination: 0.6082 - val\_loss: 1994834728.3288 - val\_coeff\_determination: 0.7371
Epoch 39/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2106467338.5205 - coeff\_determination: 0.6242 - val\_loss: 2225602233.8630 - val\_coeff\_determination: 0.7029
Epoch 40/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2232961244.9315 - coeff\_determination: 0.6059 - val\_loss: 1882787842.6301 - val\_coeff\_determination: 0.7465
Epoch 41/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2111358041.4247 - coeff\_determination: 0.6199 - val\_loss: 1985230984.7671 - val\_coeff\_determination: 0.7377
Epoch 42/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2333842419.7260 - coeff\_determination: 0.5868 - val\_loss: 2121554542.4658 - val\_coeff\_determination: 0.7161
Epoch 43/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2079142077.3699 - coeff\_determination: 0.6437 - val\_loss: 2026647235.5068 - val\_coeff\_determination: 0.7100
Epoch 44/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2148610086.5753 - coeff\_determination: 0.6159 - val\_loss: 1900350168.5479 - val\_coeff\_determination: 0.7512
Epoch 45/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2074408351.5616 - coeff\_determination: 0.6322 - val\_loss: 1837812814.9041 - val\_coeff\_determination: 0.7512
Epoch 46/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1980951785.2055 - coeff\_determination: 0.6603 - val\_loss: 1886941560.9863 - val\_coeff\_determination: 0.7354
Epoch 47/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2155713666.6301 - coeff\_determination: 0.6151 - val\_loss: 1880121301.0411 - val\_coeff\_determination: 0.7508
Epoch 48/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2029864206.0274 - coeff\_determination: 0.6404 - val\_loss: 2105628005.6986 - val\_coeff\_determination: 0.7172
Epoch 49/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2056571839.1233 - coeff\_determination: 0.6441 - val\_loss: 1820904600.5479 - val\_coeff\_determination: 0.7561
Epoch 50/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2021509063.8904 - coeff\_determination: 0.6274 - val\_loss: 1932592006.1370 - val\_coeff\_determination: 0.7256
Epoch 51/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2202904330.5205 - coeff\_determination: 0.6242 - val\_loss: 1920010532.8219 - val\_coeff\_determination: 0.7286
Epoch 52/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1986886450.8493 - coeff\_determination: 0.6644 - val\_loss: 1851044251.1781 - val\_coeff\_determination: 0.7580
Epoch 53/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2073936103.4521 - coeff\_determination: 0.6301 - val\_loss: 1803804963.0685 - val\_coeff\_determination: 0.7574
Epoch 54/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1926577662.2466 - coeff\_determination: 0.6567 - val\_loss: 1833854402.6301 - val\_coeff\_determination: 0.7592
Epoch 55/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1960491157.0411 - coeff\_determination: 0.6580 - val\_loss: 1829108913.9726 - val\_coeff\_determination: 0.7567
Epoch 56/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1951416754.8493 - coeff\_determination: 0.6516 - val\_loss: 1766948739.5068 - val\_coeff\_determination: 0.7578
Epoch 57/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2007135291.6164 - coeff\_determination: 0.6134 - val\_loss: 1803285973.0411 - val\_coeff\_determination: 0.7525
Epoch 58/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2027490827.3973 - coeff\_determination: 0.6493 - val\_loss: 1786255616.0000 - val\_coeff\_determination: 0.7563
Epoch 59/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2034964979.7260 - coeff\_determination: 0.6312 - val\_loss: 1764961429.0411 - val\_coeff\_determination: 0.7672
Epoch 60/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2162278470.1370 - coeff\_determination: 0.6218 - val\_loss: 1793577435.1781 - val\_coeff\_determination: 0.7640
Epoch 61/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2005294825.2055 - coeff\_determination: 0.6604 - val\_loss: 1753576644.3836 - val\_coeff\_determination: 0.7619
Epoch 62/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1836338703.7808 - coeff\_determination: 0.6706 - val\_loss: 1764234503.8904 - val\_coeff\_determination: 0.7668
Epoch 63/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1976240161.3151 - coeff\_determination: 0.6053 - val\_loss: 1813371471.7808 - val\_coeff\_determination: 0.7555
Epoch 64/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1722913236.1644 - coeff\_determination: 0.6815 - val\_loss: 1807635863.6712 - val\_coeff\_determination: 0.7653
Epoch 65/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1740349473.3151 - coeff\_determination: 0.6620 - val\_loss: 1785384325.2603 - val\_coeff\_determination: 0.7686
Epoch 66/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1850184072.7671 - coeff\_determination: 0.6709 - val\_loss: 1716182521.8630 - val\_coeff\_determination: 0.7748
Epoch 67/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1768458664.3288 - coeff\_determination: 0.6886 - val\_loss: 1800440863.5616 - val\_coeff\_determination: 0.7652
Epoch 68/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2102122951.8904 - coeff\_determination: 0.6465 - val\_loss: 1733835875.9452 - val\_coeff\_determination: 0.7720
Epoch 69/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1814089591.2329 - coeff\_determination: 0.6864 - val\_loss: 1749409965.5890 - val\_coeff\_determination: 0.7604
Epoch 70/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1795109851.1781 - coeff\_determination: 0.6674 - val\_loss: 1735994737.9726 - val\_coeff\_determination: 0.7712
Epoch 71/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1737810461.8082 - coeff\_determination: 0.6811 - val\_loss: 1760443830.3562 - val\_coeff\_determination: 0.7535
Epoch 72/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1753663866.7397 - coeff\_determination: 0.6815 - val\_loss: 1726334210.6301 - val\_coeff\_determination: 0.7731
Epoch 73/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1741181110.3562 - coeff\_determination: 0.6655 - val\_loss: 1754693220.8219 - val\_coeff\_determination: 0.7689
Epoch 74/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1886776691.7260 - coeff\_determination: 0.6436 - val\_loss: 1673713299.2877 - val\_coeff\_determination: 0.7746
Epoch 75/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1785832439.2329 - coeff\_determination: 0.6690 - val\_loss: 1671122595.9452 - val\_coeff\_determination: 0.7777
Epoch 76/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1763711098.7397 - coeff\_determination: 0.6790 - val\_loss: 1686773386.5205 - val\_coeff\_determination: 0.7763
Epoch 77/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1819928029.8082 - coeff\_determination: 0.6997 - val\_loss: 1825693280.4384 - val\_coeff\_determination: 0.7576
Epoch 78/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2022344988.0548 - coeff\_determination: 0.6262 - val\_loss: 1928795214.9041 - val\_coeff\_determination: 0.7419
Epoch 79/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1676496564.6027 - coeff\_determination: 0.6845 - val\_loss: 1709787019.3973 - val\_coeff\_determination: 0.7760
Epoch 80/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1933358790.1370 - coeff\_determination: 0.6704 - val\_loss: 1998096755.7260 - val\_coeff\_determination: 0.7282
Epoch 81/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1857391002.3014 - coeff\_determination: 0.6538 - val\_loss: 2035218149.6986 - val\_coeff\_determination: 0.6936
Epoch 82/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1650461971.2877 - coeff\_determination: 0.6894 - val\_loss: 1669644373.0411 - val\_coeff\_determination: 0.7756
Epoch 83/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1708976706.6301 - coeff\_determination: 0.6759 - val\_loss: 1698549636.3836 - val\_coeff\_determination: 0.7780
Epoch 84/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2007134348.2740 - coeff\_determination: 0.6228 - val\_loss: 1611773693.3699 - val\_coeff\_determination: 0.7863
Epoch 85/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1570902075.6164 - coeff\_determination: 0.7245 - val\_loss: 1640206453.4795 - val\_coeff\_determination: 0.7753
Epoch 86/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1685734817.3151 - coeff\_determination: 0.7037 - val\_loss: 1701727910.5753 - val\_coeff\_determination: 0.7645
Epoch 87/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1731328619.8356 - coeff\_determination: 0.6612 - val\_loss: 1624686452.6027 - val\_coeff\_determination: 0.7871
Epoch 88/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1636783907.0685 - coeff\_determination: 0.6937 - val\_loss: 1640220606.2466 - val\_coeff\_determination: 0.7810
Epoch 89/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1560862600.7671 - coeff\_determination: 0.7134 - val\_loss: 1639179760.2192 - val\_coeff\_determination: 0.7821
Epoch 90/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1656958588.4932 - coeff\_determination: 0.7011 - val\_loss: 1654244416.0000 - val\_coeff\_determination: 0.7829
Epoch 91/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1680734963.7260 - coeff\_determination: 0.6945 - val\_loss: 1749687791.3425 - val\_coeff\_determination: 0.7710
Epoch 92/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1828112954.7397 - coeff\_determination: 0.6683 - val\_loss: 1753692124.9315 - val\_coeff\_determination: 0.7488
Epoch 93/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1889750636.7123 - coeff\_determination: 0.6792 - val\_loss: 1846498870.3562 - val\_coeff\_determination: 0.7543
Epoch 94/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1682594272.4384 - coeff\_determination: 0.6937 - val\_loss: 1615755259.6164 - val\_coeff\_determination: 0.7884
Epoch 95/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1623217674.5205 - coeff\_determination: 0.7100 - val\_loss: 1681797112.9863 - val\_coeff\_determination: 0.7791
Epoch 96/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1638338454.7945 - coeff\_determination: 0.6985 - val\_loss: 1631652811.3973 - val\_coeff\_determination: 0.7830
Epoch 97/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1599998400.8767 - coeff\_determination: 0.6992 - val\_loss: 1944342624.4384 - val\_coeff\_determination: 0.7368
Epoch 98/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1690763050.9589 - coeff\_determination: 0.6987 - val\_loss: 1599237400.1096 - val\_coeff\_determination: 0.7825
Epoch 99/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1551776469.9178 - coeff\_determination: 0.6918 - val\_loss: 1607604979.7260 - val\_coeff\_determination: 0.7894
Epoch 100/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1634491875.9452 - coeff\_determination: 0.7116 - val\_loss: 1629002082.1918 - val\_coeff\_determination: 0.7717
Epoch 101/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1824843355.1781 - coeff\_determination: 0.6687 - val\_loss: 1593589463.2329 - val\_coeff\_determination: 0.7901
Epoch 102/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1946799182.9041 - coeff\_determination: 0.6604 - val\_loss: 1558006261.4795 - val\_coeff\_determination: 0.7949
Epoch 103/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1644287195.1781 - coeff\_determination: 0.6914 - val\_loss: 1596491910.1370 - val\_coeff\_determination: 0.7850
Epoch 104/500
1168/1168 [==============================] - 1s 1ms/step - loss: 2016666185.6438 - coeff\_determination: 0.6458 - val\_loss: 1949599644.4932 - val\_coeff\_determination: 0.7077
Epoch 105/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1642893773.1507 - coeff\_determination: 0.7007 - val\_loss: 1573582947.9452 - val\_coeff\_determination: 0.7938
Epoch 106/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1680875711.1233 - coeff\_determination: 0.6917 - val\_loss: 1549930782.6849 - val\_coeff\_determination: 0.7938
Epoch 107/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1699750296.5479 - coeff\_determination: 0.6914 - val\_loss: 1538533514.5205 - val\_coeff\_determination: 0.7922
Epoch 108/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1619540869.2603 - coeff\_determination: 0.6951 - val\_loss: 1660005103.3425 - val\_coeff\_determination: 0.7653
Epoch 109/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1668696090.3014 - coeff\_determination: 0.7047 - val\_loss: 1549206332.4932 - val\_coeff\_determination: 0.7870
Epoch 110/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1689708154.7397 - coeff\_determination: 0.6886 - val\_loss: 1557843445.4795 - val\_coeff\_determination: 0.7873
Epoch 111/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1668594379.3973 - coeff\_determination: 0.6975 - val\_loss: 1549339114.9589 - val\_coeff\_determination: 0.7901
Epoch 112/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1719316353.7534 - coeff\_determination: 0.6763 - val\_loss: 1717276083.2877 - val\_coeff\_determination: 0.7485
Epoch 113/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1856120381.3699 - coeff\_determination: 0.6437 - val\_loss: 1638533919.5616 - val\_coeff\_determination: 0.7802
Epoch 114/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1652716577.3151 - coeff\_determination: 0.6869 - val\_loss: 1537133440.8767 - val\_coeff\_determination: 0.7852
Epoch 115/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1533835383.2329 - coeff\_determination: 0.7245 - val\_loss: 1490839795.2877 - val\_coeff\_determination: 0.7988
Epoch 116/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1700203264.8767 - coeff\_determination: 0.7050 - val\_loss: 1579808136.7671 - val\_coeff\_determination: 0.7899
Epoch 117/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1684503101.3699 - coeff\_determination: 0.6775 - val\_loss: 2105004536.9863 - val\_coeff\_determination: 0.7117
Epoch 118/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1623862249.2055 - coeff\_determination: 0.7039 - val\_loss: 2049414250.9589 - val\_coeff\_determination: 0.7218
Epoch 119/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1844870689.3151 - coeff\_determination: 0.6497 - val\_loss: 1966941790.6849 - val\_coeff\_determination: 0.7274
Epoch 120/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1745561906.8493 - coeff\_determination: 0.6914 - val\_loss: 1559644490.5205 - val\_coeff\_determination: 0.7791
Epoch 121/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1524523954.8493 - coeff\_determination: 0.7291 - val\_loss: 1511224771.9452 - val\_coeff\_determination: 0.8018
Epoch 122/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1494901602.1918 - coeff\_determination: 0.7461 - val\_loss: 1514687709.1507 - val\_coeff\_determination: 0.7980
Epoch 123/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1639690892.2740 - coeff\_determination: 0.7054 - val\_loss: 1630310697.6438 - val\_coeff\_determination: 0.7618
Epoch 124/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1635866090.9589 - coeff\_determination: 0.6997 - val\_loss: 1513167412.6027 - val\_coeff\_determination: 0.7930
Epoch 125/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1369442156.7123 - coeff\_determination: 0.7244 - val\_loss: 1945334882.1918 - val\_coeff\_determination: 0.7283
Epoch 126/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1717879061.0411 - coeff\_determination: 0.6881 - val\_loss: 1952246771.7260 - val\_coeff\_determination: 0.7319
Epoch 127/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1509295949.5890 - coeff\_determination: 0.7050 - val\_loss: 1536720390.1370 - val\_coeff\_determination: 0.7894
Epoch 128/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1795963525.2603 - coeff\_determination: 0.6648 - val\_loss: 1591055308.9315 - val\_coeff\_determination: 0.7782
Epoch 129/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1450169383.4521 - coeff\_determination: 0.7384 - val\_loss: 1489309596.0548 - val\_coeff\_determination: 0.7957
Epoch 130/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1560400021.0411 - coeff\_determination: 0.7199 - val\_loss: 1545934686.2466 - val\_coeff\_determination: 0.7792
Epoch 131/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1644530835.2877 - coeff\_determination: 0.7029 - val\_loss: 1551040778.0822 - val\_coeff\_determination: 0.7971
Epoch 132/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1625468293.2603 - coeff\_determination: 0.6947 - val\_loss: 1529643803.1781 - val\_coeff\_determination: 0.7866
Epoch 133/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1608638406.1370 - coeff\_determination: 0.7281 - val\_loss: 1539211336.5479 - val\_coeff\_determination: 0.7832
Epoch 134/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1438954616.9863 - coeff\_determination: 0.7330 - val\_loss: 1484607374.9041 - val\_coeff\_determination: 0.7946
Epoch 135/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1643779252.6027 - coeff\_determination: 0.7133 - val\_loss: 1856166983.8904 - val\_coeff\_determination: 0.7206
Epoch 136/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1882377455.7808 - coeff\_determination: 0.6717 - val\_loss: 1503643529.2055 - val\_coeff\_determination: 0.7982
Epoch 137/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1436882926.4658 - coeff\_determination: 0.7224 - val\_loss: 1529345870.9041 - val\_coeff\_determination: 0.7833
Epoch 138/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1449186889.6438 - coeff\_determination: 0.7296 - val\_loss: 1551657096.5479 - val\_coeff\_determination: 0.7963
Epoch 139/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1512941294.4658 - coeff\_determination: 0.7252 - val\_loss: 1519405411.9452 - val\_coeff\_determination: 0.7889
Epoch 140/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1652180963.9452 - coeff\_determination: 0.6849 - val\_loss: 1607497632.4384 - val\_coeff\_determination: 0.7798
Epoch 141/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1390573929.2055 - coeff\_determination: 0.7283 - val\_loss: 1614250694.1370 - val\_coeff\_determination: 0.7821
Epoch 142/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1548531883.8356 - coeff\_determination: 0.7156 - val\_loss: 1542484185.4247 - val\_coeff\_determination: 0.7794
Epoch 143/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1541170730.9589 - coeff\_determination: 0.6969 - val\_loss: 1474374870.7945 - val\_coeff\_determination: 0.7974
Epoch 144/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1436336527.7808 - coeff\_determination: 0.7297 - val\_loss: 1455172643.0685 - val\_coeff\_determination: 0.8058
Epoch 145/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1417244761.4247 - coeff\_determination: 0.7360 - val\_loss: 1433377978.5205 - val\_coeff\_determination: 0.8065
Epoch 146/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1440664319.1233 - coeff\_determination: 0.7245 - val\_loss: 1429413007.7808 - val\_coeff\_determination: 0.8073
Epoch 147/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1637389436.4932 - coeff\_determination: 0.7069 - val\_loss: 1463689345.0959 - val\_coeff\_determination: 0.7990
Epoch 148/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1629219571.7260 - coeff\_determination: 0.6796 - val\_loss: 1515460807.0137 - val\_coeff\_determination: 0.7976
Epoch 149/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1731420296.3288 - coeff\_determination: 0.6939 - val\_loss: 1562394022.1370 - val\_coeff\_determination: 0.7941
Epoch 150/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1638359234.6301 - coeff\_determination: 0.7068 - val\_loss: 1850006359.6712 - val\_coeff\_determination: 0.7554
Epoch 151/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1636884257.3151 - coeff\_determination: 0.7030 - val\_loss: 1666090210.1918 - val\_coeff\_determination: 0.7730
Epoch 152/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1400995862.7945 - coeff\_determination: 0.7295 - val\_loss: 1481403190.3562 - val\_coeff\_determination: 0.8041
Epoch 153/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1421853706.5205 - coeff\_determination: 0.7460 - val\_loss: 1546251713.7534 - val\_coeff\_determination: 0.7939
Epoch 154/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1472882558.2466 - coeff\_determination: 0.7262 - val\_loss: 1491259496.1096 - val\_coeff\_determination: 0.8009
Epoch 155/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1362335089.0959 - coeff\_determination: 0.7479 - val\_loss: 1500563506.3014 - val\_coeff\_determination: 0.7866
Epoch 156/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1566085624.1096 - coeff\_determination: 0.7270 - val\_loss: 1468278928.2192 - val\_coeff\_determination: 0.7928
Epoch 157/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1705422341.2603 - coeff\_determination: 0.6965 - val\_loss: 1489137539.5068 - val\_coeff\_determination: 0.7950
Epoch 158/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1568732542.2466 - coeff\_determination: 0.7364 - val\_loss: 1433074360.5479 - val\_coeff\_determination: 0.8043
Epoch 159/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1503014005.4795 - coeff\_determination: 0.7100 - val\_loss: 1468055362.6301 - val\_coeff\_determination: 0.7953
Epoch 160/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1593117895.0137 - coeff\_determination: 0.7016 - val\_loss: 1538260223.1233 - val\_coeff\_determination: 0.7855
Epoch 161/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1513547980.2740 - coeff\_determination: 0.7305 - val\_loss: 1454796099.9452 - val\_coeff\_determination: 0.8059
Epoch 162/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1560384287.5616 - coeff\_determination: 0.7150 - val\_loss: 1514548006.5753 - val\_coeff\_determination: 0.7798
Epoch 163/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1509462094.9041 - coeff\_determination: 0.7188 - val\_loss: 1464637143.8904 - val\_coeff\_determination: 0.8009
Epoch 164/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1493382547.2877 - coeff\_determination: 0.7285 - val\_loss: 1517857823.1233 - val\_coeff\_determination: 0.7941
Epoch 165/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1565868593.0959 - coeff\_determination: 0.7015 - val\_loss: 1556648910.4658 - val\_coeff\_determination: 0.7919
Epoch 166/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1586683400.7671 - coeff\_determination: 0.6965 - val\_loss: 1556641142.7945 - val\_coeff\_determination: 0.7931
Epoch 167/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1472111644.0548 - coeff\_determination: 0.7088 - val\_loss: 1715833459.7260 - val\_coeff\_determination: 0.7682
Epoch 168/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1458871287.2329 - coeff\_determination: 0.7231 - val\_loss: 1597560846.4658 - val\_coeff\_determination: 0.7909
Epoch 169/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1381026300.4932 - coeff\_determination: 0.7351 - val\_loss: 1421486008.5479 - val\_coeff\_determination: 0.8045
Epoch 170/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1536536034.1918 - coeff\_determination: 0.7163 - val\_loss: 1878228154.7397 - val\_coeff\_determination: 0.7437
Epoch 171/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1390338968.5479 - coeff\_determination: 0.7278 - val\_loss: 1417671139.7260 - val\_coeff\_determination: 0.8042
Epoch 172/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1569017778.8493 - coeff\_determination: 0.6717 - val\_loss: 1443030000.8767 - val\_coeff\_determination: 0.8094
Epoch 173/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1578225523.7260 - coeff\_determination: 0.7068 - val\_loss: 1419129895.4521 - val\_coeff\_determination: 0.8069
Epoch 174/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1427861039.3425 - coeff\_determination: 0.7329 - val\_loss: 1518818508.9315 - val\_coeff\_determination: 0.7964
Epoch 175/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1412220345.8630 - coeff\_determination: 0.7484 - val\_loss: 1489228029.1507 - val\_coeff\_determination: 0.8011
Epoch 176/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1456233510.5753 - coeff\_determination: 0.7336 - val\_loss: 1507454051.0685 - val\_coeff\_determination: 0.7836
Epoch 177/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1340706967.6712 - coeff\_determination: 0.7365 - val\_loss: 1619955107.9452 - val\_coeff\_determination: 0.7593
Epoch 178/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1580466882.6301 - coeff\_determination: 0.6994 - val\_loss: 1727670759.8904 - val\_coeff\_determination: 0.7428
Epoch 179/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1679871246.0274 - coeff\_determination: 0.6849 - val\_loss: 1482737654.3562 - val\_coeff\_determination: 0.7906
Epoch 180/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1404044165.2603 - coeff\_determination: 0.7525 - val\_loss: 1407794440.5479 - val\_coeff\_determination: 0.8143
Epoch 181/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1398253229.5890 - coeff\_determination: 0.7448 - val\_loss: 1462293210.3014 - val\_coeff\_determination: 0.8021
Epoch 182/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1448576520.7671 - coeff\_determination: 0.7122 - val\_loss: 1413609492.3836 - val\_coeff\_determination: 0.8109
Epoch 183/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1725775842.1918 - coeff\_determination: 0.6864 - val\_loss: 1407051093.9178 - val\_coeff\_determination: 0.8106
Epoch 184/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1421602154.9589 - coeff\_determination: 0.7330 - val\_loss: 1753431610.7397 - val\_coeff\_determination: 0.7576
Epoch 185/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1530560482.1918 - coeff\_determination: 0.6950 - val\_loss: 1475236489.2055 - val\_coeff\_determination: 0.8060
Epoch 186/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1579835534.0274 - coeff\_determination: 0.7201 - val\_loss: 1408196645.0411 - val\_coeff\_determination: 0.8163
Epoch 187/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1487396634.3014 - coeff\_determination: 0.7359 - val\_loss: 1534656640.4384 - val\_coeff\_determination: 0.7947
Epoch 188/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1709446622.6849 - coeff\_determination: 0.6547 - val\_loss: 1447312179.0685 - val\_coeff\_determination: 0.7981
Epoch 189/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1530709006.0274 - coeff\_determination: 0.7289 - val\_loss: 1379501766.7945 - val\_coeff\_determination: 0.8152
Epoch 190/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1474720429.5890 - coeff\_determination: 0.7256 - val\_loss: 1434021094.3562 - val\_coeff\_determination: 0.8158
Epoch 191/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1555514895.7808 - coeff\_determination: 0.7075 - val\_loss: 1414838003.0685 - val\_coeff\_determination: 0.8053
Epoch 192/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1386027395.5068 - coeff\_determination: 0.7211 - val\_loss: 1410067742.0274 - val\_coeff\_determination: 0.8102
Epoch 193/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1536116197.6986 - coeff\_determination: 0.7217 - val\_loss: 1435219923.2877 - val\_coeff\_determination: 0.8108
Epoch 194/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1411257729.7534 - coeff\_determination: 0.7556 - val\_loss: 1529532314.7397 - val\_coeff\_determination: 0.7967
Epoch 195/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1401207317.0411 - coeff\_determination: 0.7302 - val\_loss: 1473098331.6164 - val\_coeff\_determination: 0.7887
Epoch 196/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1740797860.8219 - coeff\_determination: 0.6924 - val\_loss: 1410436102.5753 - val\_coeff\_determination: 0.8156
Epoch 197/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1609829306.7397 - coeff\_determination: 0.7013 - val\_loss: 1456594263.8904 - val\_coeff\_determination: 0.7983
Epoch 198/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1477263033.8630 - coeff\_determination: 0.7148 - val\_loss: 1392516085.4795 - val\_coeff\_determination: 0.8119
Epoch 199/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1613006541.1507 - coeff\_determination: 0.7055 - val\_loss: 1480412920.9863 - val\_coeff\_determination: 0.7829
Epoch 200/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1546838559.5616 - coeff\_determination: 0.6969 - val\_loss: 1419813146.3014 - val\_coeff\_determination: 0.8113
Epoch 201/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1758283421.8082 - coeff\_determination: 0.6466 - val\_loss: 1532955410.8493 - val\_coeff\_determination: 0.8017
Epoch 202/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1485839310.0274 - coeff\_determination: 0.7081 - val\_loss: 1469006056.3288 - val\_coeff\_determination: 0.8056
Epoch 203/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1588561015.2329 - coeff\_determination: 0.7016 - val\_loss: 1569802517.9178 - val\_coeff\_determination: 0.7822
Epoch 204/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1302301357.1507 - coeff\_determination: 0.7594 - val\_loss: 1410115351.6712 - val\_coeff\_determination: 0.7994
Epoch 205/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1440082549.4795 - coeff\_determination: 0.7286 - val\_loss: 1349455299.0685 - val\_coeff\_determination: 0.8165
Epoch 206/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1420460673.7534 - coeff\_determination: 0.7239 - val\_loss: 1535434181.2603 - val\_coeff\_determination: 0.7989
Epoch 207/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1389251046.5753 - coeff\_determination: 0.7453 - val\_loss: 1433000190.5753 - val\_coeff\_determination: 0.8013
Epoch 208/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1401703385.4247 - coeff\_determination: 0.7262 - val\_loss: 1353731178.3014 - val\_coeff\_determination: 0.8117
Epoch 209/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1496186543.3425 - coeff\_determination: 0.7384 - val\_loss: 1417123708.8219 - val\_coeff\_determination: 0.8089
Epoch 210/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1541454190.4658 - coeff\_determination: 0.7303 - val\_loss: 1448043108.6027 - val\_coeff\_determination: 0.7953
Epoch 211/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1367171493.6986 - coeff\_determination: 0.7349 - val\_loss: 1410886078.4658 - val\_coeff\_determination: 0.8111
Epoch 212/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1641218987.8356 - coeff\_determination: 0.7009 - val\_loss: 1396482681.0959 - val\_coeff\_determination: 0.8128
Epoch 213/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1707238183.4521 - coeff\_determination: 0.6690 - val\_loss: 2345953360.6575 - val\_coeff\_determination: 0.6684
Epoch 214/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1604280888.1096 - coeff\_determination: 0.7104 - val\_loss: 1651132312.1096 - val\_coeff\_determination: 0.7754
Epoch 215/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1376238187.8356 - coeff\_determination: 0.7422 - val\_loss: 1418464091.1781 - val\_coeff\_determination: 0.8115
Epoch 216/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1540586392.5479 - coeff\_determination: 0.7245 - val\_loss: 1469972023.2329 - val\_coeff\_determination: 0.8035
Epoch 217/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1354681814.7945 - coeff\_determination: 0.7297 - val\_loss: 1339710207.7808 - val\_coeff\_determination: 0.8119
Epoch 218/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1496775890.4110 - coeff\_determination: 0.7114 - val\_loss: 1432611857.7534 - val\_coeff\_determination: 0.8025
Epoch 219/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1375752982.7945 - coeff\_determination: 0.7407 - val\_loss: 1461553873.7534 - val\_coeff\_determination: 0.8012
Epoch 220/500
1168/1168 [==============================] - 2s 1ms/step - loss: 1421701372.4932 - coeff\_determination: 0.7282 - val\_loss: 1400609421.1507 - val\_coeff\_determination: 0.8063
Epoch 221/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1474716082.8493 - coeff\_determination: 0.7194 - val\_loss: 1392909189.5342 - val\_coeff\_determination: 0.8151
Epoch 222/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1594313607.0137 - coeff\_determination: 0.7148 - val\_loss: 1493978137.8630 - val\_coeff\_determination: 0.7780
Epoch 223/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1499328731.1781 - coeff\_determination: 0.7266 - val\_loss: 1398128742.5753 - val\_coeff\_determination: 0.8127
Epoch 224/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1349620702.6849 - coeff\_determination: 0.7553 - val\_loss: 1352857032.3288 - val\_coeff\_determination: 0.8106
Epoch 225/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1570288468.1644 - coeff\_determination: 0.7230 - val\_loss: 1463627805.3699 - val\_coeff\_determination: 0.7896
Epoch 226/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1596305515.8356 - coeff\_determination: 0.7252 - val\_loss: 1432415776.2192 - val\_coeff\_determination: 0.7898
Epoch 227/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1366900736.0000 - coeff\_determination: 0.7431 - val\_loss: 1496898250.9589 - val\_coeff\_determination: 0.8035
Epoch 228/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1573891962.7397 - coeff\_determination: 0.7098 - val\_loss: 1372329264.7671 - val\_coeff\_determination: 0.8161
Epoch 229/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1264296732.0548 - coeff\_determination: 0.7771 - val\_loss: 1330545798.5753 - val\_coeff\_determination: 0.8166
Epoch 230/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1395587035.1781 - coeff\_determination: 0.7255 - val\_loss: 1740047649.3151 - val\_coeff\_determination: 0.7688
Epoch 231/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1477409781.4795 - coeff\_determination: 0.7331 - val\_loss: 1389642451.9452 - val\_coeff\_determination: 0.8147
Epoch 232/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1522355664.6575 - coeff\_determination: 0.7365 - val\_loss: 1337218750.0274 - val\_coeff\_determination: 0.8184
Epoch 233/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1522282294.3562 - coeff\_determination: 0.6949 - val\_loss: 1564865555.2877 - val\_coeff\_determination: 0.7695
Epoch 234/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1561298307.5068 - coeff\_determination: 0.6834 - val\_loss: 1369500548.8219 - val\_coeff\_determination: 0.8142
Epoch 235/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1639328895.1233 - coeff\_determination: 0.7104 - val\_loss: 1398446404.2740 - val\_coeff\_determination: 0.8050
Epoch 236/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1361964138.9589 - coeff\_determination: 0.7352 - val\_loss: 1635334475.8356 - val\_coeff\_determination: 0.7793
Epoch 237/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1494662841.8630 - coeff\_determination: 0.7350 - val\_loss: 1396395061.9178 - val\_coeff\_determination: 0.8087
Epoch 238/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1338892529.9726 - coeff\_determination: 0.7574 - val\_loss: 1362796016.2192 - val\_coeff\_determination: 0.8136
Epoch 239/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1280248770.6301 - coeff\_determination: 0.7781 - val\_loss: 1534726762.9589 - val\_coeff\_determination: 0.7885
Epoch 240/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1518198409.6438 - coeff\_determination: 0.7117 - val\_loss: 1378554442.9589 - val\_coeff\_determination: 0.8012
Epoch 241/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1403194430.2466 - coeff\_determination: 0.7465 - val\_loss: 1361082493.3699 - val\_coeff\_determination: 0.8197
Epoch 242/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1519494568.3288 - coeff\_determination: 0.7183 - val\_loss: 1593757123.5068 - val\_coeff\_determination: 0.7883
Epoch 243/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1566161487.7808 - coeff\_determination: 0.7229 - val\_loss: 1350589727.7260 - val\_coeff\_determination: 0.8193
Epoch 244/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1291607022.4658 - coeff\_determination: 0.7546 - val\_loss: 1346288339.8356 - val\_coeff\_determination: 0.8192
Epoch 245/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1307353144.1096 - coeff\_determination: 0.7526 - val\_loss: 1385965231.4521 - val\_coeff\_determination: 0.8066
Epoch 246/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1378178801.9726 - coeff\_determination: 0.7363 - val\_loss: 1509957045.4795 - val\_coeff\_determination: 0.7862
Epoch 247/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1340690488.1096 - coeff\_determination: 0.7331 - val\_loss: 1494381496.3288 - val\_coeff\_determination: 0.7954
Epoch 248/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1521094671.7808 - coeff\_determination: 0.7106 - val\_loss: 1314272047.3425 - val\_coeff\_determination: 0.8134
Epoch 249/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1459576069.2603 - coeff\_determination: 0.7347 - val\_loss: 1577823292.4932 - val\_coeff\_determination: 0.7807
Epoch 250/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1457619716.3836 - coeff\_determination: 0.7313 - val\_loss: 1356790431.3425 - val\_coeff\_determination: 0.8129
Epoch 251/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1308337949.8082 - coeff\_determination: 0.7660 - val\_loss: 1428229616.0000 - val\_coeff\_determination: 0.8037
Epoch 252/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1400126673.5342 - coeff\_determination: 0.7421 - val\_loss: 1382338523.6712 - val\_coeff\_determination: 0.8090
Epoch 253/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1292808666.3014 - coeff\_determination: 0.7645 - val\_loss: 1356590047.7808 - val\_coeff\_determination: 0.8210
Epoch 254/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1545320379.6164 - coeff\_determination: 0.7152 - val\_loss: 1427616279.3425 - val\_coeff\_determination: 0.7951
Epoch 255/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1481718538.5205 - coeff\_determination: 0.7385 - val\_loss: 1327911491.2877 - val\_coeff\_determination: 0.8165
Epoch 256/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1408243494.5753 - coeff\_determination: 0.7375 - val\_loss: 1495503075.0685 - val\_coeff\_determination: 0.7967
Epoch 257/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1480954055.8904 - coeff\_determination: 0.7228 - val\_loss: 1435245990.3562 - val\_coeff\_determination: 0.8006
Epoch 258/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1444725565.3699 - coeff\_determination: 0.7266 - val\_loss: 1365387214.6849 - val\_coeff\_determination: 0.8085
Epoch 259/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1381131470.9041 - coeff\_determination: 0.7340 - val\_loss: 1362332903.8904 - val\_coeff\_determination: 0.8155
Epoch 260/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1333222155.3973 - coeff\_determination: 0.7482 - val\_loss: 1406228951.1233 - val\_coeff\_determination: 0.7968
Epoch 261/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1352086417.5342 - coeff\_determination: 0.7379 - val\_loss: 1397738739.0685 - val\_coeff\_determination: 0.7916
Epoch 262/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1340207044.3836 - coeff\_determination: 0.7479 - val\_loss: 1376148443.7260 - val\_coeff\_determination: 0.7993
Epoch 263/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1457877243.6164 - coeff\_determination: 0.7334 - val\_loss: 1478206080.0000 - val\_coeff\_determination: 0.8025
Epoch 264/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1386338107.6164 - coeff\_determination: 0.7370 - val\_loss: 1339521565.1507 - val\_coeff\_determination: 0.8142
Epoch 265/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1396367718.5753 - coeff\_determination: 0.7368 - val\_loss: 1375026872.7671 - val\_coeff\_determination: 0.8187
Epoch 266/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1395066087.4521 - coeff\_determination: 0.7383 - val\_loss: 1452708772.3836 - val\_coeff\_determination: 0.8017
Epoch 267/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1413323935.5616 - coeff\_determination: 0.7361 - val\_loss: 1409320255.1233 - val\_coeff\_determination: 0.8148
Epoch 268/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1433893670.5753 - coeff\_determination: 0.7425 - val\_loss: 1369338465.3151 - val\_coeff\_determination: 0.8139
Epoch 269/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1353520697.8630 - coeff\_determination: 0.7509 - val\_loss: 1312038128.6575 - val\_coeff\_determination: 0.8101
Epoch 270/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1213515175.4521 - coeff\_determination: 0.7800 - val\_loss: 1464372873.6438 - val\_coeff\_determination: 0.8045
Epoch 271/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1241220723.7260 - coeff\_determination: 0.7763 - val\_loss: 1339770420.6027 - val\_coeff\_determination: 0.8047
Epoch 272/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1490679001.4247 - coeff\_determination: 0.7155 - val\_loss: 1339362267.8356 - val\_coeff\_determination: 0.8176
Epoch 273/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1346450775.6712 - coeff\_determination: 0.7385 - val\_loss: 1411117833.2055 - val\_coeff\_determination: 0.7932
Epoch 274/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1487602215.4521 - coeff\_determination: 0.7206 - val\_loss: 1317369616.7671 - val\_coeff\_determination: 0.8245
Epoch 275/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1371315520.8767 - coeff\_determination: 0.7528 - val\_loss: 1304872735.1233 - val\_coeff\_determination: 0.8181
Epoch 276/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1490883676.9315 - coeff\_determination: 0.7355 - val\_loss: 1311821004.7123 - val\_coeff\_determination: 0.8147
Epoch 277/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1306024767.1233 - coeff\_determination: 0.7467 - val\_loss: 1413464101.6986 - val\_coeff\_determination: 0.8132
Epoch 278/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1296062038.7945 - coeff\_determination: 0.7450 - val\_loss: 1337885045.4795 - val\_coeff\_determination: 0.8186
Epoch 279/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1432718712.9863 - coeff\_determination: 0.7322 - val\_loss: 1342645420.7123 - val\_coeff\_determination: 0.8066
Epoch 280/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1335165876.6027 - coeff\_determination: 0.7546 - val\_loss: 1349229834.5205 - val\_coeff\_determination: 0.8178
Epoch 281/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1192060405.4795 - coeff\_determination: 0.7614 - val\_loss: 1315278191.4521 - val\_coeff\_determination: 0.8177
Epoch 282/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1311128464.6575 - coeff\_determination: 0.7569 - val\_loss: 1532216396.9315 - val\_coeff\_determination: 0.7991
Epoch 283/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1326803411.2877 - coeff\_determination: 0.7592 - val\_loss: 1485575539.2877 - val\_coeff\_determination: 0.8059
Epoch 284/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1441740639.5616 - coeff\_determination: 0.7275 - val\_loss: 1519080981.9178 - val\_coeff\_determination: 0.8010
Epoch 285/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1367229815.2329 - coeff\_determination: 0.7573 - val\_loss: 1324831432.9863 - val\_coeff\_determination: 0.8178
Epoch 286/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1361856725.9178 - coeff\_determination: 0.7573 - val\_loss: 1442714211.7260 - val\_coeff\_determination: 0.7881
Epoch 287/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1294221496.1096 - coeff\_determination: 0.7666 - val\_loss: 1335322444.4932 - val\_coeff\_determination: 0.8159
Epoch 288/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1383163861.9178 - coeff\_determination: 0.7591 - val\_loss: 1850161500.0548 - val\_coeff\_determination: 0.7416
Epoch 289/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1292312263.8904 - coeff\_determination: 0.7663 - val\_loss: 1290451661.6986 - val\_coeff\_determination: 0.8193
Epoch 290/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1321424575.1233 - coeff\_determination: 0.7382 - val\_loss: 1417129526.1370 - val\_coeff\_determination: 0.8071
Epoch 291/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1558338778.3014 - coeff\_determination: 0.7117 - val\_loss: 1341583495.2329 - val\_coeff\_determination: 0.8123
Epoch 292/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1682536370.8493 - coeff\_determination: 0.7068 - val\_loss: 1403375902.6849 - val\_coeff\_determination: 0.8063
Epoch 293/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1709219487.5616 - coeff\_determination: 0.7142 - val\_loss: 1362050885.6986 - val\_coeff\_determination: 0.8147
Epoch 294/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1432515186.8493 - coeff\_determination: 0.7441 - val\_loss: 1357275766.7945 - val\_coeff\_determination: 0.7973
Epoch 295/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1335977819.1781 - coeff\_determination: 0.7396 - val\_loss: 1563606188.2740 - val\_coeff\_determination: 0.7857
Epoch 296/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1421675278.0274 - coeff\_determination: 0.7278 - val\_loss: 1369849722.7397 - val\_coeff\_determination: 0.8042
Epoch 297/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1389277390.9041 - coeff\_determination: 0.7490 - val\_loss: 1320178757.2603 - val\_coeff\_determination: 0.8233
Epoch 298/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1435498518.7945 - coeff\_determination: 0.7530 - val\_loss: 1277860246.1370 - val\_coeff\_determination: 0.8250
Epoch 299/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1294661516.2740 - coeff\_determination: 0.7540 - val\_loss: 1359556511.5616 - val\_coeff\_determination: 0.8027
Epoch 300/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1442458352.2192 - coeff\_determination: 0.7250 - val\_loss: 1429992273.5342 - val\_coeff\_determination: 0.7952
Epoch 301/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1371721249.3151 - coeff\_determination: 0.7429 - val\_loss: 1247810425.4247 - val\_coeff\_determination: 0.8201
Epoch 302/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1368043157.0411 - coeff\_determination: 0.7590 - val\_loss: 1517404448.0000 - val\_coeff\_determination: 0.7934
Epoch 303/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1318788704.4384 - coeff\_determination: 0.7453 - val\_loss: 1272151266.8493 - val\_coeff\_determination: 0.8212
Epoch 304/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1397194291.7260 - coeff\_determination: 0.7486 - val\_loss: 1930897376.4384 - val\_coeff\_determination: 0.7167
Epoch 305/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1484098489.8630 - coeff\_determination: 0.7093 - val\_loss: 1325737924.8219 - val\_coeff\_determination: 0.8237
Epoch 306/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1493968575.1233 - coeff\_determination: 0.7284 - val\_loss: 1404665586.6301 - val\_coeff\_determination: 0.8189
Epoch 307/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1313910406.1370 - coeff\_determination: 0.7499 - val\_loss: 1385263523.9452 - val\_coeff\_determination: 0.8146
Epoch 308/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1280914186.5205 - coeff\_determination: 0.7672 - val\_loss: 1639824503.2329 - val\_coeff\_determination: 0.7703
Epoch 309/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1387257305.4247 - coeff\_determination: 0.7564 - val\_loss: 1312215432.3288 - val\_coeff\_determination: 0.8124
Epoch 310/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1344540822.7945 - coeff\_determination: 0.7411 - val\_loss: 1261998852.8219 - val\_coeff\_determination: 0.8214
Epoch 311/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1289044056.5479 - coeff\_determination: 0.7490 - val\_loss: 1337892606.6849 - val\_coeff\_determination: 0.8047
Epoch 312/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1374596848.2192 - coeff\_determination: 0.7362 - val\_loss: 1368387763.7260 - val\_coeff\_determination: 0.8111
Epoch 313/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1455217881.4247 - coeff\_determination: 0.7427 - val\_loss: 1313189067.7260 - val\_coeff\_determination: 0.8213
Epoch 314/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1354626054.1370 - coeff\_determination: 0.7407 - val\_loss: 1756602740.6027 - val\_coeff\_determination: 0.7522
Epoch 315/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1384784881.0959 - coeff\_determination: 0.7508 - val\_loss: 1411645839.1233 - val\_coeff\_determination: 0.8096
Epoch 316/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1434119802.7397 - coeff\_determination: 0.7446 - val\_loss: 1318552832.4384 - val\_coeff\_determination: 0.8160
Epoch 317/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1330328824.9863 - coeff\_determination: 0.7309 - val\_loss: 1358647208.3288 - val\_coeff\_determination: 0.8172
Epoch 318/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1316450030.4658 - coeff\_determination: 0.7569 - val\_loss: 1328608525.1507 - val\_coeff\_determination: 0.8200
Epoch 319/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1440870014.2466 - coeff\_determination: 0.7315 - val\_loss: 1522306613.9178 - val\_coeff\_determination: 0.7834
Epoch 320/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1309365717.9178 - coeff\_determination: 0.7567 - val\_loss: 1419244498.4110 - val\_coeff\_determination: 0.8081
Epoch 321/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1347564647.4521 - coeff\_determination: 0.7539 - val\_loss: 1307558343.4521 - val\_coeff\_determination: 0.8090
Epoch 322/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1281668401.0959 - coeff\_determination: 0.7705 - val\_loss: 1367167098.7397 - val\_coeff\_determination: 0.8034
Epoch 323/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1327074062.0274 - coeff\_determination: 0.7408 - val\_loss: 1361653468.4932 - val\_coeff\_determination: 0.8066
Epoch 324/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1329970589.8082 - coeff\_determination: 0.7398 - val\_loss: 1626092611.0685 - val\_coeff\_determination: 0.7672
Epoch 325/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1320243040.4384 - coeff\_determination: 0.7471 - val\_loss: 1500673219.0685 - val\_coeff\_determination: 0.8005
Epoch 326/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1454259662.9041 - coeff\_determination: 0.7350 - val\_loss: 1354317237.0411 - val\_coeff\_determination: 0.8056
Epoch 327/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1288666485.4795 - coeff\_determination: 0.7509 - val\_loss: 1317054940.9315 - val\_coeff\_determination: 0.8175
Epoch 328/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1203585465.8630 - coeff\_determination: 0.7706 - val\_loss: 1312542311.4521 - val\_coeff\_determination: 0.8113
Epoch 329/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1185037445.2603 - coeff\_determination: 0.7785 - val\_loss: 1357316892.6027 - val\_coeff\_determination: 0.8066
Epoch 330/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1297324053.9178 - coeff\_determination: 0.7425 - val\_loss: 1312474638.3562 - val\_coeff\_determination: 0.8162
Epoch 331/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1434656392.7671 - coeff\_determination: 0.7307 - val\_loss: 1490602498.6301 - val\_coeff\_determination: 0.7793
Epoch 332/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1375593298.4110 - coeff\_determination: 0.7194 - val\_loss: 1335695395.0685 - val\_coeff\_determination: 0.8058
Epoch 333/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1431685314.6301 - coeff\_determination: 0.7132 - val\_loss: 1251849346.8493 - val\_coeff\_determination: 0.8169
Epoch 334/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1332940809.6438 - coeff\_determination: 0.7516 - val\_loss: 1331794553.6438 - val\_coeff\_determination: 0.8149
Epoch 335/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1313479825.5342 - coeff\_determination: 0.7663 - val\_loss: 1420650196.1644 - val\_coeff\_determination: 0.7812
Epoch 336/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1384434734.4658 - coeff\_determination: 0.7357 - val\_loss: 1345907761.3151 - val\_coeff\_determination: 0.8121
Epoch 337/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1280781496.1096 - coeff\_determination: 0.7393 - val\_loss: 1344014478.0274 - val\_coeff\_determination: 0.8075
Epoch 338/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1457307578.3014 - coeff\_determination: 0.7317 - val\_loss: 1487355434.9589 - val\_coeff\_determination: 0.7999
Epoch 339/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1141219963.6164 - coeff\_determination: 0.7816 - val\_loss: 1293252584.0000 - val\_coeff\_determination: 0.8136
Epoch 340/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1269143818.5205 - coeff\_determination: 0.7498 - val\_loss: 1282637152.0000 - val\_coeff\_determination: 0.8181
Epoch 341/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1255453901.1507 - coeff\_determination: 0.7693 - val\_loss: 1353921471.5616 - val\_coeff\_determination: 0.8144
Epoch 342/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1324800915.7260 - coeff\_determination: 0.7497 - val\_loss: 1501521556.1644 - val\_coeff\_determination: 0.7847
Epoch 343/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1332068336.2192 - coeff\_determination: 0.7582 - val\_loss: 1332782286.4658 - val\_coeff\_determination: 0.8116
Epoch 344/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1316548995.5068 - coeff\_determination: 0.7429 - val\_loss: 1214299528.7671 - val\_coeff\_determination: 0.8250
Epoch 345/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1343635575.2329 - coeff\_determination: 0.7569 - val\_loss: 1310656060.4932 - val\_coeff\_determination: 0.8092
Epoch 346/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1332446839.2329 - coeff\_determination: 0.7581 - val\_loss: 1604319719.4521 - val\_coeff\_determination: 0.7579
Epoch 347/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1196667301.6986 - coeff\_determination: 0.7860 - val\_loss: 1261538176.8767 - val\_coeff\_determination: 0.8120
Epoch 348/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1246450391.6712 - coeff\_determination: 0.7651 - val\_loss: 1244737522.6301 - val\_coeff\_determination: 0.8229
Epoch 349/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1393698977.3151 - coeff\_determination: 0.7344 - val\_loss: 1269639676.4932 - val\_coeff\_determination: 0.8087
Epoch 350/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1298129052.0548 - coeff\_determination: 0.7644 - val\_loss: 1395842044.4932 - val\_coeff\_determination: 0.8103
Epoch 351/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1310446406.1370 - coeff\_determination: 0.7578 - val\_loss: 1305006430.2466 - val\_coeff\_determination: 0.8214
Epoch 352/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1206112763.6164 - coeff\_determination: 0.7611 - val\_loss: 1307030229.0411 - val\_coeff\_determination: 0.8068
Epoch 353/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1482689737.6438 - coeff\_determination: 0.7072 - val\_loss: 1370940141.1507 - val\_coeff\_determination: 0.8017
Epoch 354/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1249795208.7671 - coeff\_determination: 0.7772 - val\_loss: 1312591448.5479 - val\_coeff\_determination: 0.7983
Epoch 355/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1224230247.4521 - coeff\_determination: 0.7724 - val\_loss: 1282641830.5753 - val\_coeff\_determination: 0.8238
Epoch 356/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1350375198.6849 - coeff\_determination: 0.7532 - val\_loss: 1405476312.9863 - val\_coeff\_determination: 0.7848
Epoch 357/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1389717899.3973 - coeff\_determination: 0.7447 - val\_loss: 1279534540.2740 - val\_coeff\_determination: 0.8189
Epoch 358/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1179231915.8356 - coeff\_determination: 0.7781 - val\_loss: 1204661556.3836 - val\_coeff\_determination: 0.8203
Epoch 359/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1376427683.0685 - coeff\_determination: 0.7259 - val\_loss: 1516312654.0274 - val\_coeff\_determination: 0.7897
Epoch 360/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1301530616.9863 - coeff\_determination: 0.7541 - val\_loss: 1309115530.5205 - val\_coeff\_determination: 0.8094
Epoch 361/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1240002347.8356 - coeff\_determination: 0.7645 - val\_loss: 1320304705.5342 - val\_coeff\_determination: 0.8105
Epoch 362/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1262993383.4521 - coeff\_determination: 0.7492 - val\_loss: 1224644130.6301 - val\_coeff\_determination: 0.8258
Epoch 363/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1317488497.9726 - coeff\_determination: 0.7473 - val\_loss: 1336165968.4384 - val\_coeff\_determination: 0.8100
Epoch 364/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1436294477.1507 - coeff\_determination: 0.7433 - val\_loss: 1219517098.7397 - val\_coeff\_determination: 0.8184
Epoch 365/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1358068907.8356 - coeff\_determination: 0.7353 - val\_loss: 1372055341.5890 - val\_coeff\_determination: 0.8121
Epoch 366/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1156032597.9178 - coeff\_determination: 0.7939 - val\_loss: 1263891363.0685 - val\_coeff\_determination: 0.8114
Epoch 367/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1155692616.7671 - coeff\_determination: 0.7676 - val\_loss: 1238743267.2877 - val\_coeff\_determination: 0.8245
Epoch 368/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1224446456.9863 - coeff\_determination: 0.7735 - val\_loss: 1309306857.3151 - val\_coeff\_determination: 0.8232
Epoch 369/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1403562339.0685 - coeff\_determination: 0.7512 - val\_loss: 1328627778.6301 - val\_coeff\_determination: 0.7972
Epoch 370/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1438712113.9726 - coeff\_determination: 0.7238 - val\_loss: 1194525608.5479 - val\_coeff\_determination: 0.8236
Epoch 371/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1480413837.1507 - coeff\_determination: 0.7328 - val\_loss: 1247573174.7945 - val\_coeff\_determination: 0.8178
Epoch 372/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1171452560.6575 - coeff\_determination: 0.7872 - val\_loss: 1231761077.2603 - val\_coeff\_determination: 0.8107
Epoch 373/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1446998608.6575 - coeff\_determination: 0.7246 - val\_loss: 1299005068.7123 - val\_coeff\_determination: 0.8121
Epoch 374/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1224720085.9178 - coeff\_determination: 0.7657 - val\_loss: 1276579330.9589 - val\_coeff\_determination: 0.8124
Epoch 375/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1406740575.5616 - coeff\_determination: 0.7442 - val\_loss: 1315499150.4658 - val\_coeff\_determination: 0.8187
Epoch 376/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1208178603.3973 - coeff\_determination: 0.7763 - val\_loss: 1278256174.9041 - val\_coeff\_determination: 0.8171
Epoch 377/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1214839304.7671 - coeff\_determination: 0.7747 - val\_loss: 1263107790.4658 - val\_coeff\_determination: 0.8226
Epoch 378/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1231648676.8219 - coeff\_determination: 0.7720 - val\_loss: 1289641066.5205 - val\_coeff\_determination: 0.8135
Epoch 379/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1287403883.8356 - coeff\_determination: 0.7567 - val\_loss: 1415624380.4932 - val\_coeff\_determination: 0.7846
Epoch 380/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1311760496.2192 - coeff\_determination: 0.7670 - val\_loss: 1429125110.7945 - val\_coeff\_determination: 0.8146
Epoch 381/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1279947574.3562 - coeff\_determination: 0.7816 - val\_loss: 1293707366.1370 - val\_coeff\_determination: 0.8159
Epoch 382/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1211967701.9178 - coeff\_determination: 0.7657 - val\_loss: 1316188353.0959 - val\_coeff\_determination: 0.8088
Epoch 383/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1372342549.0411 - coeff\_determination: 0.7480 - val\_loss: 1257476668.2740 - val\_coeff\_determination: 0.8179
Epoch 384/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1280731194.7397 - coeff\_determination: 0.7603 - val\_loss: 1349631303.8904 - val\_coeff\_determination: 0.8190
Epoch 385/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1242140170.5205 - coeff\_determination: 0.7627 - val\_loss: 1326828661.9178 - val\_coeff\_determination: 0.8136
Epoch 386/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1185756705.3151 - coeff\_determination: 0.7751 - val\_loss: 1320113449.8630 - val\_coeff\_determination: 0.8088
Epoch 387/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1288932515.0685 - coeff\_determination: 0.7495 - val\_loss: 1304064902.5753 - val\_coeff\_determination: 0.8168
Epoch 388/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1268384746.9589 - coeff\_determination: 0.7740 - val\_loss: 1284773440.4384 - val\_coeff\_determination: 0.8183
Epoch 389/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1152584595.2877 - coeff\_determination: 0.7872 - val\_loss: 1241887743.1233 - val\_coeff\_determination: 0.8188
Epoch 390/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1321436708.8219 - coeff\_determination: 0.7438 - val\_loss: 1329684149.0411 - val\_coeff\_determination: 0.8054
Epoch 391/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1217007061.9178 - coeff\_determination: 0.7739 - val\_loss: 1292329561.8630 - val\_coeff\_determination: 0.8188
Epoch 392/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1297091813.6986 - coeff\_determination: 0.7561 - val\_loss: 1318062472.7671 - val\_coeff\_determination: 0.8090
Epoch 393/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1380879300.3836 - coeff\_determination: 0.7268 - val\_loss: 1268119502.9041 - val\_coeff\_determination: 0.8063
Epoch 394/500
1168/1168 [==============================] - 2s 1ms/step - loss: 1214396731.6164 - coeff\_determination: 0.7636 - val\_loss: 1234727783.8904 - val\_coeff\_determination: 0.8190
Epoch 395/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1390519806.2466 - coeff\_determination: 0.7362 - val\_loss: 1349916382.2466 - val\_coeff\_determination: 0.7979
Epoch 396/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1335986047.1233 - coeff\_determination: 0.7633 - val\_loss: 1262603603.2877 - val\_coeff\_determination: 0.8173
Epoch 397/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1117827418.3014 - coeff\_determination: 0.7842 - val\_loss: 1262845806.9041 - val\_coeff\_determination: 0.8131
Epoch 398/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1141055374.0274 - coeff\_determination: 0.7853 - val\_loss: 1323584948.3836 - val\_coeff\_determination: 0.8154
Epoch 399/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1340877596.0548 - coeff\_determination: 0.7443 - val\_loss: 1247831576.5479 - val\_coeff\_determination: 0.8195
Epoch 400/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1227993899.8356 - coeff\_determination: 0.7757 - val\_loss: 1315134535.4521 - val\_coeff\_determination: 0.8018
Epoch 401/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1271176648.7671 - coeff\_determination: 0.7644 - val\_loss: 1443444672.4384 - val\_coeff\_determination: 0.7931
Epoch 402/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1310619772.4932 - coeff\_determination: 0.7572 - val\_loss: 1393403212.2740 - val\_coeff\_determination: 0.7925
Epoch 403/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1411791593.2055 - coeff\_determination: 0.7311 - val\_loss: 1630820314.3014 - val\_coeff\_determination: 0.7674
Epoch 404/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1360551977.2055 - coeff\_determination: 0.7442 - val\_loss: 1631455363.0685 - val\_coeff\_determination: 0.7673
Epoch 405/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1162346375.8904 - coeff\_determination: 0.7783 - val\_loss: 1397446301.3699 - val\_coeff\_determination: 0.8012
Epoch 406/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1093164956.0548 - coeff\_determination: 0.7933 - val\_loss: 1338417756.4932 - val\_coeff\_determination: 0.8099
Epoch 407/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1099885909.9178 - coeff\_determination: 0.7861 - val\_loss: 1354782654.6849 - val\_coeff\_determination: 0.8144
Epoch 408/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1186216129.7534 - coeff\_determination: 0.7764 - val\_loss: 1223400265.4247 - val\_coeff\_determination: 0.8165
Epoch 409/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1134852944.6575 - coeff\_determination: 0.7797 - val\_loss: 1199886151.8904 - val\_coeff\_determination: 0.8176
Epoch 410/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1319632608.4384 - coeff\_determination: 0.7575 - val\_loss: 1231418031.3425 - val\_coeff\_determination: 0.8170
Epoch 411/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1276884039.8904 - coeff\_determination: 0.7527 - val\_loss: 1450696272.4384 - val\_coeff\_determination: 0.7992
Epoch 412/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1221347658.0822 - coeff\_determination: 0.7687 - val\_loss: 1308807246.0274 - val\_coeff\_determination: 0.8165
Epoch 413/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1296941435.5068 - coeff\_determination: 0.7707 - val\_loss: 1198231224.5479 - val\_coeff\_determination: 0.8232
Epoch 414/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1174618232.9863 - coeff\_determination: 0.7819 - val\_loss: 1254623587.9452 - val\_coeff\_determination: 0.8091
Epoch 415/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1132574395.6164 - coeff\_determination: 0.7790 - val\_loss: 1355778504.1096 - val\_coeff\_determination: 0.7955
Epoch 416/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1276178025.2055 - coeff\_determination: 0.7644 - val\_loss: 1468426330.7397 - val\_coeff\_determination: 0.8012
Epoch 417/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1285700683.3973 - coeff\_determination: 0.7544 - val\_loss: 1269186617.4247 - val\_coeff\_determination: 0.8153
Epoch 418/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1336820888.5479 - coeff\_determination: 0.7602 - val\_loss: 1261264122.7397 - val\_coeff\_determination: 0.8074
Epoch 419/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1265769306.3014 - coeff\_determination: 0.7391 - val\_loss: 1241258318.9041 - val\_coeff\_determination: 0.8134
Epoch 420/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1293218887.8904 - coeff\_determination: 0.7583 - val\_loss: 1418543962.3014 - val\_coeff\_determination: 0.7938
Epoch 421/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1342204523.8356 - coeff\_determination: 0.7519 - val\_loss: 1389228176.0000 - val\_coeff\_determination: 0.8054
Epoch 422/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1257289047.6712 - coeff\_determination: 0.7621 - val\_loss: 1216790389.0411 - val\_coeff\_determination: 0.8174
Epoch 423/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1268992643.5068 - coeff\_determination: 0.7574 - val\_loss: 1301865226.0822 - val\_coeff\_determination: 0.8079
Epoch 424/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1234165042.8493 - coeff\_determination: 0.7681 - val\_loss: 1307636821.9178 - val\_coeff\_determination: 0.8114
Epoch 425/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1254241960.3288 - coeff\_determination: 0.7710 - val\_loss: 1300551878.3562 - val\_coeff\_determination: 0.8157
Epoch 426/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1223839393.3151 - coeff\_determination: 0.7750 - val\_loss: 1342592829.1507 - val\_coeff\_determination: 0.8185
Epoch 427/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1182679338.9589 - coeff\_determination: 0.7720 - val\_loss: 1326731780.3836 - val\_coeff\_determination: 0.8168
Epoch 428/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1202341940.6027 - coeff\_determination: 0.7452 - val\_loss: 1384003985.7534 - val\_coeff\_determination: 0.8054
Epoch 429/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1444427369.2055 - coeff\_determination: 0.7394 - val\_loss: 1228318292.6027 - val\_coeff\_determination: 0.8218
Epoch 430/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1164074332.9315 - coeff\_determination: 0.7797 - val\_loss: 1271284899.9452 - val\_coeff\_determination: 0.8122
Epoch 431/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1314679632.6575 - coeff\_determination: 0.7535 - val\_loss: 1408742589.3699 - val\_coeff\_determination: 0.7851
Epoch 432/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1326234846.6849 - coeff\_determination: 0.7584 - val\_loss: 1243867503.3425 - val\_coeff\_determination: 0.8138
Epoch 433/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1403758229.0411 - coeff\_determination: 0.7473 - val\_loss: 1260592954.3014 - val\_coeff\_determination: 0.8144
Epoch 434/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1180244380.9315 - coeff\_determination: 0.7789 - val\_loss: 1188900482.6301 - val\_coeff\_determination: 0.8254
Epoch 435/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1217538902.7945 - coeff\_determination: 0.7725 - val\_loss: 1290663513.2055 - val\_coeff\_determination: 0.8153
Epoch 436/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1290561274.7397 - coeff\_determination: 0.7537 - val\_loss: 1388277909.9178 - val\_coeff\_determination: 0.7985
Epoch 437/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1230326405.2603 - coeff\_determination: 0.7699 - val\_loss: 1256196351.1233 - val\_coeff\_determination: 0.8076
Epoch 438/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1275576497.0959 - coeff\_determination: 0.7569 - val\_loss: 1306297609.8630 - val\_coeff\_determination: 0.8041
Epoch 439/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1268130979.0685 - coeff\_determination: 0.7510 - val\_loss: 1252138064.4384 - val\_coeff\_determination: 0.8150
Epoch 440/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1252638914.6301 - coeff\_determination: 0.7477 - val\_loss: 1272717710.0274 - val\_coeff\_determination: 0.8063
Epoch 441/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1139437445.2603 - coeff\_determination: 0.7793 - val\_loss: 1286512141.8082 - val\_coeff\_determination: 0.8201
Epoch 442/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1240329086.2466 - coeff\_determination: 0.7602 - val\_loss: 1265757508.3836 - val\_coeff\_determination: 0.8139
Epoch 443/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1325137245.8082 - coeff\_determination: 0.7601 - val\_loss: 1315388210.8493 - val\_coeff\_determination: 0.8116
Epoch 444/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1211861919.5616 - coeff\_determination: 0.7621 - val\_loss: 1331079279.3425 - val\_coeff\_determination: 0.8114
Epoch 445/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1409162081.3151 - coeff\_determination: 0.7381 - val\_loss: 1242914023.4521 - val\_coeff\_determination: 0.8110
Epoch 446/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1352843172.8219 - coeff\_determination: 0.7407 - val\_loss: 1280380672.0000 - val\_coeff\_determination: 0.7945
Epoch 447/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1311658743.2329 - coeff\_determination: 0.7566 - val\_loss: 1365586956.4932 - val\_coeff\_determination: 0.8091
Epoch 448/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1325464094.6849 - coeff\_determination: 0.7587 - val\_loss: 1220240762.5205 - val\_coeff\_determination: 0.8126
Epoch 449/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1491130106.7397 - coeff\_determination: 0.7221 - val\_loss: 1226025828.6027 - val\_coeff\_determination: 0.8191
Epoch 450/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1376422765.5890 - coeff\_determination: 0.7429 - val\_loss: 1225804194.4110 - val\_coeff\_determination: 0.8218
Epoch 451/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1357691093.9178 - coeff\_determination: 0.7376 - val\_loss: 1464607551.1233 - val\_coeff\_determination: 0.7849
Epoch 452/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1222038608.6575 - coeff\_determination: 0.7756 - val\_loss: 1389295126.7945 - val\_coeff\_determination: 0.7966
Epoch 453/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1238370510.9041 - coeff\_determination: 0.7699 - val\_loss: 1299952436.6027 - val\_coeff\_determination: 0.8086
Epoch 454/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1391498913.3151 - coeff\_determination: 0.7499 - val\_loss: 1339334148.8219 - val\_coeff\_determination: 0.8096
Epoch 455/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1223397813.4795 - coeff\_determination: 0.7673 - val\_loss: 1308569851.6164 - val\_coeff\_determination: 0.8130
Epoch 456/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1185494901.4795 - coeff\_determination: 0.7749 - val\_loss: 1474089453.5890 - val\_coeff\_determination: 0.7814
Epoch 457/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1258032825.8630 - coeff\_determination: 0.7578 - val\_loss: 1208353426.4110 - val\_coeff\_determination: 0.8226
Epoch 458/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1236451487.5616 - coeff\_determination: 0.7695 - val\_loss: 1228076138.7397 - val\_coeff\_determination: 0.8204
Epoch 459/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1409655162.7397 - coeff\_determination: 0.7478 - val\_loss: 1334708646.5753 - val\_coeff\_determination: 0.8056
Epoch 460/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1179883750.5753 - coeff\_determination: 0.7763 - val\_loss: 1385396464.2192 - val\_coeff\_determination: 0.7967
Epoch 461/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1276835308.7123 - coeff\_determination: 0.7683 - val\_loss: 1338391617.3151 - val\_coeff\_determination: 0.8084
Epoch 462/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1224011094.7945 - coeff\_determination: 0.7722 - val\_loss: 1230462951.0137 - val\_coeff\_determination: 0.8190
Epoch 463/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1235618820.3836 - coeff\_determination: 0.7716 - val\_loss: 1333496191.1233 - val\_coeff\_determination: 0.8101
Epoch 464/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1175450321.9726 - coeff\_determination: 0.7719 - val\_loss: 1333561806.4658 - val\_coeff\_determination: 0.8009
Epoch 465/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1231304630.3562 - coeff\_determination: 0.7601 - val\_loss: 1511556073.2055 - val\_coeff\_determination: 0.7794
Epoch 466/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1136913611.3973 - coeff\_determination: 0.7861 - val\_loss: 1326023208.3288 - val\_coeff\_determination: 0.8169
Epoch 467/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1224260512.4384 - coeff\_determination: 0.7811 - val\_loss: 1308182990.2466 - val\_coeff\_determination: 0.8192
Epoch 468/500
1168/1168 [==============================] - 2s 1ms/step - loss: 1211992318.2466 - coeff\_determination: 0.7651 - val\_loss: 1276161856.8767 - val\_coeff\_determination: 0.8069
Epoch 469/500
1168/1168 [==============================] - 2s 1ms/step - loss: 1346491253.4795 - coeff\_determination: 0.7448 - val\_loss: 1356366556.9315 - val\_coeff\_determination: 0.8012
Epoch 470/500
1168/1168 [==============================] - 2s 1ms/step - loss: 1205993009.0959 - coeff\_determination: 0.7706 - val\_loss: 1273916154.3014 - val\_coeff\_determination: 0.8113
Epoch 471/500
1168/1168 [==============================] - 2s 1ms/step - loss: 1274770377.6438 - coeff\_determination: 0.7454 - val\_loss: 1268613712.2192 - val\_coeff\_determination: 0.8200
Epoch 472/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1149840461.1507 - coeff\_determination: 0.7735 - val\_loss: 1230757084.4932 - val\_coeff\_determination: 0.8149
Epoch 473/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1218711285.4795 - coeff\_determination: 0.7756 - val\_loss: 1288749608.1096 - val\_coeff\_determination: 0.8171
Epoch 474/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1235555633.0959 - coeff\_determination: 0.7792 - val\_loss: 1331722538.5205 - val\_coeff\_determination: 0.7928
Epoch 475/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1162266086.5753 - coeff\_determination: 0.7750 - val\_loss: 1349185809.0959 - val\_coeff\_determination: 0.7936
Epoch 476/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1188511051.3973 - coeff\_determination: 0.7726 - val\_loss: 1452736670.6849 - val\_coeff\_determination: 0.7843
Epoch 477/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1270186177.7534 - coeff\_determination: 0.7532 - val\_loss: 1532141055.1233 - val\_coeff\_determination: 0.7851
Epoch 478/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1119929722.7397 - coeff\_determination: 0.7781 - val\_loss: 1465473316.8219 - val\_coeff\_determination: 0.7836
Epoch 479/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1274543859.7260 - coeff\_determination: 0.7628 - val\_loss: 1186099620.8219 - val\_coeff\_determination: 0.8178
Epoch 480/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1233003058.8493 - coeff\_determination: 0.7797 - val\_loss: 1409555003.1781 - val\_coeff\_determination: 0.8037
Epoch 481/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1234723391.1233 - coeff\_determination: 0.7705 - val\_loss: 1322580309.9178 - val\_coeff\_determination: 0.8095
Epoch 482/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1289798343.8904 - coeff\_determination: 0.7621 - val\_loss: 1245829284.6027 - val\_coeff\_determination: 0.8120
Epoch 483/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1227594150.5753 - coeff\_determination: 0.7684 - val\_loss: 1237896374.7945 - val\_coeff\_determination: 0.8157
Epoch 484/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1224415405.5890 - coeff\_determination: 0.7735 - val\_loss: 1278549940.6027 - val\_coeff\_determination: 0.8109
Epoch 485/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1073216686.4658 - coeff\_determination: 0.7937 - val\_loss: 1294957873.0959 - val\_coeff\_determination: 0.8137
Epoch 486/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1031432298.9589 - coeff\_determination: 0.8015 - val\_loss: 1307389269.0411 - val\_coeff\_determination: 0.8027
Epoch 487/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1104282076.9315 - coeff\_determination: 0.7872 - val\_loss: 1320644149.0411 - val\_coeff\_determination: 0.8108
Epoch 488/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1307285454.0274 - coeff\_determination: 0.7460 - val\_loss: 1264825507.5068 - val\_coeff\_determination: 0.7998
Epoch 489/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1317402525.8082 - coeff\_determination: 0.7520 - val\_loss: 1346986086.5753 - val\_coeff\_determination: 0.7966
Epoch 490/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1212164531.7260 - coeff\_determination: 0.7683 - val\_loss: 1285087710.2466 - val\_coeff\_determination: 0.8158
Epoch 491/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1184614053.6986 - coeff\_determination: 0.7846 - val\_loss: 1351259715.5068 - val\_coeff\_determination: 0.7899
Epoch 492/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1434347164.0548 - coeff\_determination: 0.7253 - val\_loss: 1478784353.3151 - val\_coeff\_determination: 0.7806
Epoch 493/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1275518340.3836 - coeff\_determination: 0.7529 - val\_loss: 1299304227.9452 - val\_coeff\_determination: 0.7913
Epoch 494/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1233475039.5616 - coeff\_determination: 0.7537 - val\_loss: 1308088000.8767 - val\_coeff\_determination: 0.8080
Epoch 495/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1177175514.3014 - coeff\_determination: 0.7771 - val\_loss: 1437277903.3425 - val\_coeff\_determination: 0.7841
Epoch 496/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1128862592.0000 - coeff\_determination: 0.7819 - val\_loss: 1151962558.2466 - val\_coeff\_determination: 0.8164
Epoch 497/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1141736482.1918 - coeff\_determination: 0.7967 - val\_loss: 1297058133.9178 - val\_coeff\_determination: 0.8141
Epoch 498/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1235769557.0411 - coeff\_determination: 0.7682 - val\_loss: 1194848782.4658 - val\_coeff\_determination: 0.8175
Epoch 499/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1451552810.9589 - coeff\_determination: 0.7141 - val\_loss: 1280807997.8082 - val\_coeff\_determination: 0.8091
Epoch 500/500
1168/1168 [==============================] - 1s 1ms/step - loss: 1198341558.3562 - coeff\_determination: 0.7782 - val\_loss: 1202758983.4521 - val\_coeff\_determination: 0.8103

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}41}]:} <keras.callbacks.History at 0x1f903ed2668>
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:}  \PY{n}{prediction}\PY{o}{=}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{testSet1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{p}{(}\PY{n}{prediction}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}43}]:} array([[ 115712.0703125],
                [ 166315.140625 ],
                [ 187738.234375 ],
                {\ldots}, 
                [ 174498.328125 ],
                [ 130424.421875 ],
                [ 235921.421875 ]], dtype=float32)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{n}{prediction\PYZus{}df}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{(}\PY{n}{prediction}\PY{p}{)}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SalePrice}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{predictedTest}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{test}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Id}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{prediction\PYZus{}df}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{predictedTest}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Keras\PYZus{}Submission.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \textbf{KAGGLE SCORE}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{Image}
         \PY{n}{IM}\PY{o}{=}\PY{n}{Image}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Keras\PYZus{}submission.png}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{IM}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}46}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_44_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \section{Designing Ensemble}\label{designing-ensemble}

Importing Required Libraries

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{k+kn}{import} \PY{n+nn}{datetime}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cross\PYZus{}validation} \PY{k}{import} \PY{n}{KFold}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cross\PYZus{}validation} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         \PY{k+kn}{import} \PY{n+nn}{time}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing}
         \PY{k+kn}{from} \PY{n+nn}{xgboost} \PY{k}{import} \PY{n}{XGBRegressor}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestRegressor}\PY{p}{,} \PY{n}{ExtraTreesRegressor}\PY{p}{,} \PY{n}{GradientBoostingRegressor}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{grid\PYZus{}search} \PY{k}{import} \PY{n}{GridSearchCV}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cross\PYZus{}validation} \PY{k}{import} \PY{n}{ShuffleSplit}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{make\PYZus{}scorer}\PY{p}{,} \PY{n}{mean\PYZus{}squared\PYZus{}error}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{Ridge}\PY{p}{,} \PY{n}{LassoCV}\PY{p}{,}\PY{n}{LassoLarsCV}\PY{p}{,} \PY{n}{ElasticNet}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{kernel\PYZus{}ridge} \PY{k}{import} \PY{n}{KernelRidge}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsRegressor}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVR}
         \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k}{import} \PY{n}{skew}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}grid\_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model\_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
  DeprecationWarning)

    \end{Verbatim}

    \textbf{Build A Model Library}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{n}{train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./input/train.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} read train data}
         \PY{n}{test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./input/test.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} read test data}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{n}{base\PYZus{}models} \PY{o}{=} \PY{p}{[}
                 \PY{n}{RandomForestRegressor}\PY{p}{(}
                     \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
                     \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{14}
                 \PY{p}{)}\PY{p}{,}
                 \PY{n}{RandomForestRegressor}\PY{p}{(}
                     \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
                     \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
         	    \PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{7}
                 \PY{p}{)}\PY{p}{,}
                 \PY{n}{ExtraTreesRegressor}\PY{p}{(}
                     \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                     \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{15}
                 \PY{p}{)}\PY{p}{,}
                 \PY{n}{ExtraTreesRegressor}\PY{p}{(}
                     \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                   \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{20}
                 \PY{p}{)}\PY{p}{,}
                 \PY{n}{GradientBoostingRegressor}\PY{p}{(}
                     \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                     \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{,}
                     \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{subsample}\PY{o}{=}\PY{l+m+mf}{0.8}
                 \PY{p}{)}\PY{p}{,}
         	\PY{n}{GradientBoostingRegressor}\PY{p}{(}
                     \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                     \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{,}
                     \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{subsample}\PY{o}{=}\PY{l+m+mf}{0.8}
                 \PY{p}{)}\PY{p}{,}
                 \PY{n}{XGBRegressor}\PY{p}{(}
                     \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
                     \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,}
                     \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{subsample}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{n}{colsample\PYZus{}bytree}\PY{o}{=}\PY{l+m+mf}{0.75}
                 \PY{p}{)}\PY{p}{,}
          
                 \PY{n}{XGBRegressor}\PY{p}{(}
                     \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
                     \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,}
                     \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{subsample}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{n}{colsample\PYZus{}bytree}\PY{o}{=}\PY{l+m+mf}{0.75}
                 \PY{p}{)}\PY{p}{,}
         	\PY{n}{LassoCV}\PY{p}{(}\PY{n}{alphas} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.0005}\PY{p}{]}\PY{p}{)}\PY{p}{,}
         	\PY{n}{KNeighborsRegressor}\PY{p}{(}\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                	\PY{n}{KNeighborsRegressor}\PY{p}{(}\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,}
               	\PY{n}{KNeighborsRegressor}\PY{p}{(}\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{,}
                 \PY{n}{KNeighborsRegressor}\PY{p}{(}\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{l+m+mi}{25}\PY{p}{)}\PY{p}{,}
         	\PY{n}{LassoLarsCV}\PY{p}{(}\PY{p}{)}\PY{p}{,}
         	\PY{n}{ElasticNet}\PY{p}{(}\PY{p}{)}\PY{p}{,}
         	\PY{n}{SVR}\PY{p}{(}\PY{p}{)}
             \PY{p}{]}
\end{Verbatim}


    \textbf{Data Preprocess}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{k}{def} \PY{n+nf}{data\PYZus{}preprocess}\PY{p}{(}\PY{n}{train}\PY{p}{,}\PY{n}{test}\PY{p}{)}\PY{p}{:}
             \PY{n}{outlier\PYZus{}idx} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{11}\PY{p}{,}\PY{l+m+mi}{13}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{46}\PY{p}{,}\PY{l+m+mi}{66}\PY{p}{,}\PY{l+m+mi}{70}\PY{p}{,}\PY{l+m+mi}{167}\PY{p}{,}\PY{l+m+mi}{178}\PY{p}{,}\PY{l+m+mi}{185}\PY{p}{,}\PY{l+m+mi}{199}\PY{p}{,} \PY{l+m+mi}{224}\PY{p}{,}\PY{l+m+mi}{261}\PY{p}{,} \PY{l+m+mi}{309}\PY{p}{,}\PY{l+m+mi}{313}\PY{p}{,}\PY{l+m+mi}{318}\PY{p}{,} \PY{l+m+mi}{349}\PY{p}{,}\PY{l+m+mi}{412}\PY{p}{,}\PY{l+m+mi}{423}\PY{p}{,}\PY{l+m+mi}{440}\PY{p}{,}\PY{l+m+mi}{454}\PY{p}{,}\PY{l+m+mi}{477}\PY{p}{,}\PY{l+m+mi}{478}\PY{p}{,} \PY{l+m+mi}{523}\PY{p}{,}\PY{l+m+mi}{540}\PY{p}{,} \PY{l+m+mi}{581}\PY{p}{,}\PY{l+m+mi}{588}\PY{p}{,}\PY{l+m+mi}{595}\PY{p}{,}\PY{l+m+mi}{654}\PY{p}{,}\PY{l+m+mi}{688}\PY{p}{,} \PY{l+m+mi}{691}\PY{p}{,} \PY{l+m+mi}{774}\PY{p}{,} \PY{l+m+mi}{798}\PY{p}{,} \PY{l+m+mi}{875}\PY{p}{,} \PY{l+m+mi}{898}\PY{p}{,}\PY{l+m+mi}{926}\PY{p}{,}\PY{l+m+mi}{970}\PY{p}{,}\PY{l+m+mi}{987}\PY{p}{,}\PY{l+m+mi}{1027}\PY{p}{,}\PY{l+m+mi}{1109}\PY{p}{,} \PY{l+m+mi}{1169}\PY{p}{,}\PY{l+m+mi}{1182}\PY{p}{,}\PY{l+m+mi}{1239}\PY{p}{,} \PY{l+m+mi}{1256}\PY{p}{,}\PY{l+m+mi}{1298}\PY{p}{,}\PY{l+m+mi}{1324}\PY{p}{,}\PY{l+m+mi}{1353}\PY{p}{,}\PY{l+m+mi}{1359}\PY{p}{,}\PY{l+m+mi}{1405}\PY{p}{,}\PY{l+m+mi}{1442}\PY{p}{,}\PY{l+m+mi}{1447}\PY{p}{]}
             \PY{n}{train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{n}{outlier\PYZus{}idx}\PY{p}{]}\PY{p}{,}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{all\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSSubClass}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SaleCondition}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                                   \PY{n}{test}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSSubClass}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SaleCondition}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{to\PYZus{}delete} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Alley}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{FireplaceQu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PoolQC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fence}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MiscFeature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{n}{all\PYZus{}data} \PY{o}{=} \PY{n}{all\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{to\PYZus{}delete}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
             \PY{n}{train}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SalePrice}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log1p}\PY{p}{(}\PY{n}{train}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SalePrice}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}log transform skewed numeric features}
             \PY{n}{numeric\PYZus{}feats} \PY{o}{=} \PY{n}{all\PYZus{}data}\PY{o}{.}\PY{n}{dtypes}\PY{p}{[}\PY{n}{all\PYZus{}data}\PY{o}{.}\PY{n}{dtypes} \PY{o}{!=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{object}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{index}
             \PY{n}{skewed\PYZus{}feats} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{n}{numeric\PYZus{}feats}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{skew}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}compute skewness}
             \PY{n}{skewed\PYZus{}feats} \PY{o}{=} \PY{n}{skewed\PYZus{}feats}\PY{p}{[}\PY{n}{skewed\PYZus{}feats} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.75}\PY{p}{]}
             \PY{n}{skewed\PYZus{}feats} \PY{o}{=} \PY{n}{skewed\PYZus{}feats}\PY{o}{.}\PY{n}{index}
             \PY{n}{all\PYZus{}data}\PY{p}{[}\PY{n}{skewed\PYZus{}feats}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log1p}\PY{p}{(}\PY{n}{all\PYZus{}data}\PY{p}{[}\PY{n}{skewed\PYZus{}feats}\PY{p}{]}\PY{p}{)}
             \PY{n}{all\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{get\PYZus{}dummies}\PY{p}{(}\PY{n}{all\PYZus{}data}\PY{p}{)}
             \PY{n}{all\PYZus{}data} \PY{o}{=} \PY{n}{all\PYZus{}data}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{all\PYZus{}data}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{all\PYZus{}data}\PY{p}{[}\PY{p}{:}\PY{n}{train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}
             \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{all\PYZus{}data}\PY{p}{[}\PY{n}{train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}\PY{p}{]}
             \PY{n}{y} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{SalePrice}
         
             \PY{k}{return} \PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{k}{def} \PY{n+nf}{mean\PYZus{}squared\PYZus{}error\PYZus{}}\PY{p}{(}\PY{n}{ground\PYZus{}truth}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{ground\PYZus{}truth}\PY{p}{,} \PY{n}{predictions}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mf}{0.5}
         \PY{n}{RMSE} \PY{o}{=} \PY{n}{make\PYZus{}scorer}\PY{p}{(}\PY{n}{mean\PYZus{}squared\PYZus{}error\PYZus{}}\PY{p}{,} \PY{n}{greater\PYZus{}is\PYZus{}better}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{k}{class} \PY{n+nc}{ensemble}\PY{p}{(}\PY{n+nb}{object}\PY{p}{)}\PY{p}{:}
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{n\PYZus{}folds}\PY{p}{,} \PY{n}{stacker}\PY{p}{,} \PY{n}{base\PYZus{}models}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}folds} \PY{o}{=} \PY{n}{n\PYZus{}folds}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{stacker} \PY{o}{=} \PY{n}{stacker}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}models} \PY{o}{=} \PY{n}{base\PYZus{}models}
             \PY{k}{def} \PY{n+nf}{fit\PYZus{}predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{train}\PY{p}{,}\PY{n}{test}\PY{p}{,}\PY{n}{ytr}\PY{p}{)}\PY{p}{:}
                 \PY{n}{X} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{values}
                 \PY{n}{y} \PY{o}{=} \PY{n}{ytr}\PY{o}{.}\PY{n}{values}
                 \PY{n}{T} \PY{o}{=} \PY{n}{test}\PY{o}{.}\PY{n}{values}
                 \PY{n}{folds} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{KFold}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}folds} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}folds}\PY{p}{,} \PY{n}{shuffle} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,} \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
                 \PY{n}{S\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}models}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                 \PY{n}{S\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{T}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}models}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
                 \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{reg} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{base\PYZus{}models}\PY{p}{)}\PY{p}{:}
                     \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fitting the base model...}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                     \PY{n}{S\PYZus{}test\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{T}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{folds}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
                     \PY{k}{for} \PY{n}{j}\PY{p}{,} \PY{p}{(}\PY{n}{train\PYZus{}idx}\PY{p}{,}\PY{n}{test\PYZus{}idx}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{folds}\PY{p}{)}\PY{p}{:}
                         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}
                         \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}
                         \PY{n}{X\PYZus{}holdout} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]}
                         \PY{n}{reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
                         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}holdout}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{]}
                         \PY{n}{S\PYZus{}train}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}pred}
                         \PY{n}{S\PYZus{}test\PYZus{}i}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{T}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{]}
                     \PY{n}{S\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{S\PYZus{}test\PYZus{}i}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
                  
                 \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Stacking base models...}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} tuning the stacker}
                 \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{,}\PY{l+m+mf}{5e\PYZhy{}3}\PY{p}{,}\PY{l+m+mf}{1e\PYZhy{}2}\PY{p}{,}\PY{l+m+mf}{5e\PYZhy{}2}\PY{p}{,}\PY{l+m+mf}{1e\PYZhy{}1}\PY{p}{,}\PY{l+m+mf}{0.2}\PY{p}{,}\PY{l+m+mf}{0.3}\PY{p}{,}\PY{l+m+mf}{0.4}\PY{p}{,}\PY{l+m+mf}{0.5}\PY{p}{,}\PY{l+m+mf}{0.8}\PY{p}{,}\PY{l+m+mf}{1e0}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mf}{1e1}\PY{p}{]}\PY{p}{,}\PY{p}{\PYZcb{}}
                 \PY{n}{grid} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{estimator}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{stacker}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{n}{RMSE}\PY{p}{)}
                 \PY{n}{grid}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{S\PYZus{}train}\PY{p}{,} \PY{n}{y}\PY{p}{)}
                 \PY{k}{try}\PY{p}{:}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Param grid:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{n}{param\PYZus{}grid}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best Params:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best CV Score:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best estimator:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(}\PY{n}{message}\PY{p}{)}
                 \PY{k}{except}\PY{p}{:}
                     \PY{k}{pass}
         
                 \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{grid}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{S\PYZus{}test}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{]}
                 \PY{k}{return} \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} \PY{n}{ensem} \PY{o}{=} \PY{n}{ensemble}\PY{p}{(}
                 \PY{n}{n\PYZus{}folds}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
         	\PY{n}{stacker}\PY{o}{=}\PY{n}{Ridge}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                 \PY{n}{base\PYZus{}models}\PY{o}{=}\PY{n}{base\PYZus{}models}
             \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{data\PYZus{}preprocess}\PY{p}{(}\PY{n}{train}\PY{p}{,}\PY{n}{test}\PY{p}{)}
         \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{n}{score} \PY{o}{=} \PY{n}{ensem}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Fitting the base model{\ldots}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.578e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.833e-04, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.482e-04, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.557e-04, with an active set of 75 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 80 iterations, alpha=2.523e-04, previous alpha=2.500e-04, with an active set of 77 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.872e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.307e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 168 iterations, alpha=9.552e-05, previous alpha=8.399e-05, with an active set of 147 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.159e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.003e-04, with an active set of 122 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.003e-04, with an active set of 122 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.003e-04, with an active set of 122 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 130 iterations, alpha=9.711e-05, previous alpha=9.632e-05, with an active set of 125 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.209e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.612e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.044e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.987e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=2.370e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=4.509e-05, with an active set of 157 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=4.509e-05, with an active set of 157 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 183 iterations, alpha=4.224e-05, previous alpha=4.194e-05, with an active set of 160 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=1.240e-04, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.051e-04, with an active set of 106 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=9.705e-05, with an active set of 115 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=9.133e-05, with an active set of 120 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=7.388e-05, with an active set of 131 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=5.255e-05, with an active set of 147 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 171 iterations, alpha=4.482e-05, previous alpha=4.446e-05, with an active set of 156 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.945e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=7.937e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.730e-04, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 107 iterations, alpha=1.517e-04, previous alpha=1.401e-04, with an active set of 102 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.726e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.826e-04, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 80 iterations, alpha=1.760e-04, previous alpha=1.742e-04, with an active set of 73 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.872e-04, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.530e-04, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.530e-04, with an active set of 72 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.530e-04, with an active set of 72 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.964e-04, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.343e-04, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.264e-04, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.264e-04, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.264e-04, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.264e-04, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.012e-04, with an active set of 108 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=9.903e-05, with an active set of 110 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=9.153e-05, with an active set of 115 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=9.140e-05, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 141 iterations, alpha=7.248e-05, previous alpha=7.191e-05, with an active set of 126 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.510e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.236e-04, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.979e-04, with an active set of 75 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=1.590e-04, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.189e-04, with an active set of 106 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 128 iterations, alpha=9.923e-05, previous alpha=9.902e-05, with an active set of 117 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=3.821e-04, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.171e-04, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=1.522e-04, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=1.522e-04, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=1.512e-04, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 127 iterations, alpha=1.209e-04, previous alpha=1.089e-04, with an active set of 116 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.825e-04, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=9.824e-05, with an active set of 111 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.540e-05, with an active set of 114 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=5.154e-05, with an active set of 150 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=5.154e-05, with an active set of 150 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=5.154e-05, with an active set of 150 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 168 iterations, alpha=4.870e-05, previous alpha=4.763e-05, with an active set of 153 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.474e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.308e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.308e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=4.405e-04, previous alpha=4.386e-04, with an active set of 45 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.146e-04, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=3.146e-04, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=3.146e-04, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=2.208e-04, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.161e-04, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.122e-04, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.028e-04, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 87 iterations, alpha=1.917e-04, previous alpha=1.866e-04, with an active set of 84 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.224e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.782e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.015e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 114 iterations, alpha=1.464e-04, previous alpha=1.440e-04, with an active set of 103 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.170e-04, with an active set of 110 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.100e-04, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.062e-04, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=4.437e-05, with an active set of 149 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=4.437e-05, with an active set of 149 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=4.427e-05, with an active set of 149 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 184 iterations, i.e. alpha=3.496e-05, with an active set of 160 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 187 iterations, i.e. alpha=3.471e-05, with an active set of 163 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 193 iterations, i.e. alpha=3.140e-05, with an active set of 167 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 193 iterations, i.e. alpha=3.100e-05, with an active set of 167 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 203 iterations, alpha=2.955e-05, previous alpha=2.815e-05, with an active set of 176 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=7.242e-05, with an active set of 141 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=7.202e-05, with an active set of 142 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=7.202e-05, with an active set of 142 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=7.202e-05, with an active set of 142 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 186 iterations, i.e. alpha=3.601e-05, with an active set of 166 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 186 iterations, i.e. alpha=3.601e-05, with an active set of 166 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 197 iterations, i.e. alpha=2.865e-05, with an active set of 177 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 213 iterations, i.e. alpha=1.801e-05, with an active set of 187 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 217 iterations, alpha=1.642e-05, previous alpha=1.634e-05, with an active set of 190 regressors.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.317e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.912e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)
F:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}linear\_model\textbackslash{}least\_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=2.533e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max\_iter or increase eps parameters.
  ConvergenceWarning)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Fitting the base model{\ldots}
Fitting the base model{\ldots}
Stacking base models{\ldots}
Param grid:
\{'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.8, 1.0, 3, 5, 7, 10.0]\}
Best Params:
\{'alpha': 0.5\}
Best CV Score:
0.1006822219728786
Best estimator:
Ridge(alpha=0.5, copy\_X=True, fit\_intercept=True, max\_iter=None,
   normalize=False, random\_state=None, solver='auto', tol=0.001)

    \end{Verbatim}

    \textbf{Create Submission}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{k}{def} \PY{n+nf}{create\PYZus{}submission}\PY{p}{(}\PY{n}{prediction}\PY{p}{,}\PY{n}{score}\PY{p}{)}\PY{p}{:}
             \PY{n}{now} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}
             \PY{n}{sub\PYZus{}file} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{submission\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{score}\PY{p}{)}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{now}\PY{o}{.}\PY{n}{strftime}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{Y\PYZhy{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{m\PYZhy{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZhy{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{H\PYZhy{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{M}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.csv}\PY{l+s+s1}{\PYZsq{}}
             \PY{c+c1}{\PYZsh{}sub\PYZus{}file = \PYZsq{}prediction\PYZus{}training.csv\PYZsq{}}
             \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Creating submission: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{sub\PYZus{}file}\PY{p}{)}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{prediction}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{n}{sub\PYZus{}file}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{n}{create\PYZus{}submission}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expm1}\PY{p}{(}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{,}\PY{n}{score}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Creating submission:  submission\_0.1006822219728786\_2018-08-13-12-48.csv

    \end{Verbatim}

    \textbf{KAGGLE SCORE}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{Image}
         \PY{n}{IM}\PY{o}{=}\PY{n}{Image}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ensemble\PYZus{}submission.png}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{IM}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}68}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \section{Conclusion}\label{conclusion}

    First Model was used in the neural network (Keras) to obtain a Kaggle
score of about 0.14878.

    First Model was used in Ensemble (Random Forest \& XGBooster) to obtain
a Kaggle score of about 0.11722.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
